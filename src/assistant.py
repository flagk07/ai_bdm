from __future__ import annotations

from typing import Any, Dict, List, Tuple
from datetime import date, datetime, timedelta

from openai import OpenAI

from .config import get_settings
from .pii import sanitize_text, sanitize_text_assistant_output
from .db import Database
import re
import os
from typing import Optional


ALLOWED_TOPICS_HINT = (
	"–±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã; –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏; —Å–∫—Ä–∏–ø—Ç—ã; —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞; —Ü–µ–ª–∏; –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π"
)


def _chat_completion_with_fallback(client: OpenAI, model: str, messages: List[Dict[str, str]], temperature: float, max_tokens: int) -> str:
	"""Call Chat Completions with a safe fallback to gpt-4o-mini if the model is unavailable.
	- Triggers fallback on common provider errors like model_not_found/404/does not exist.
	"""
	try:
		resp = client.chat.completions.create(
			model=model,
			messages=messages,
			temperature=temperature,
			max_tokens=max_tokens,
		)
		return resp.choices[0].message.content or ""
	except Exception as exc:
		emsg = str(exc).lower()
		if ("model_not_found" in emsg) or ("does not exist" in emsg) or ("404" in emsg):
			# Fallback model tuned for speed/cost
			resp = client.chat.completions.create(
				model="gpt-4o-mini",
				messages=messages,
				temperature=temperature,
				max_tokens=max_tokens,
			)
			return resp.choices[0].message.content or ""
		raise


# ------------------------ Deposit rates helpers ------------------------

def _parse_amount_rub(text: str) -> Optional[float]:
	low = text.lower().replace("\u00a0", " ").replace("\u202f", " ")
	# 1) Explicit currency forms
	m = re.search(r"(\d[\d\s]{2,}(?:[.,]\d{1,2})?)\s*(?:—Ä—É–±|‚ÇΩ|rub)", low)
	if m:
		num = m.group(1).replace(" ", "").replace(",", ".")
		try:
			return float(num)
		except Exception:
			return None
	# 2) Word-based multipliers (–º–ª–Ω/—Ç—ã—Å) without currency
	m2 = re.search(r"(\d+(?:[.,]\d+)?)\s*(–º–ª–Ω|–º–∏–ª–ª–∏–æ–Ω|million|m|—Ç—ã—Å|—Ç—ã—Å—è—á|k)\b", low)
	if m2:
		val = float(m2.group(1).replace(",", "."))
		unit = m2.group(2)
		mult = 1.0
		if unit.startswith("–º–ª") or unit.startswith("mil") or unit == "m":
			mult = 1_000_000.0
		elif unit.startswith("—Ç—ã—Å") or unit.startswith("k"):
			mult = 1_000.0
		return val * mult
	# 3) Bare number likely representing RUB (>=5 digits)
	m3 = re.search(r"\b(\d[\d\s]{4,})\b", low)
	if m3:
		try:
			return float(m3.group(1).replace(" ", ""))
		except Exception:
			return None
	return None


def _parse_payout_type(text: str) -> Optional[str]:
	low = text.lower()
	if "–µ–∂–µ–º–µ—Å—è—á" in low or "–∫–∞–∂–¥—ã–π –º–µ—Å—è—Ü" in low:
		return "monthly"
	if "–≤ –∫–æ–Ω—Ü–µ" in low or "–ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏" in low or "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü" in low:
		return "end"
	return None


def _parse_term_days(text: str) -> Optional[int]:
	low = text.lower()
	# handle colloquial half-year
	if "–ø–æ–ª–≥–æ–¥–∞" in low or "–ø–æ–ª –≥–æ–¥–∞" in low or "–ø–æ–ª-–≥–æ–¥" in low:
		return 181
	# months mapping (legacy fallback)
	mon_map = {1:31,2:61,3:91,4:122,6:181,9:274,12:367,18:550,24:730,36:1100}
	m_mon = re.search(r"(\d+)\s*(?:–º–µ—Å|–º–µ—Å—è—Ü|–º–µ—Å—è—Ü–∞|–º–µ—Å—è—Ü–µ–≤)\b", low)
	if m_mon:
		mon = int(m_mon.group(1))
		return mon_map.get(mon, mon * 30)
	m_day = re.search(r"(\d+)\s*(?:–¥–Ω|–¥–Ω–µ–π|day|days)\b", low)
	if m_day:
		return int(m_day.group(1))
	# plain number that looks like days
	m_num = re.search(r"\b(\d{2,4})\b", low)
	if m_num:
		val = int(m_num.group(1))
		if 10 <= val <= 2000:
			return val
	return None


def _parse_term_days_smart(text: str, db: Database, product: str = "–í–∫–ª–∞–¥") -> Optional[int]:
	m = re.search(r"(\d+)\s*(?:–º–µ—Å|–º–µ—Å—è—Ü|–º–µ—Å—è—Ü–∞|–º–µ—Å—è—Ü–µ–≤)\b", text.lower())
	if not m:
		return _parse_term_days(text)
	wanted = int(m.group(1)) * 30
	terms = db.distinct_terms(product)
	if not terms:
		return None
	return min(terms, key=lambda d: abs(d - wanted))


def _detect_preferences(text: str) -> Dict[str, Any]:
	low = text.lower()
	prefs: Dict[str, Any] = {}
	if any(k in low for k in ["—Å—Ç–∞–≤–∫ –ø–æ–≤—ã—à–µ", "—Å—Ç–∞–≤–∫ –ø–æ–≤—ã—à–µ", "—Å—Ç–∞–≤–∫–∞ –≤—ã—à–µ", "—Å—Ç–∞–≤–∫—É –≤—ã—à–µ", "–ø–æ–≤—ã—à–µ", "–≤—ã—à–µ", "–±–æ–ª—å—à–µ —Å—Ç–∞–≤–∫–∞", "—Å—Ç–∞–≤–∫–∞ –ø–æ–±–æ–ª—å—à–µ"]):
		prefs["rate"] = "high"
	elif any(k in low for k in ["–ø–æ–º–µ–Ω—å—à–µ", "–Ω–∏–∂–µ —Å—Ç–∞–≤–∫–∞", "—Å—Ç–∞–≤–∫–∞ –Ω–∏–∂–µ", "—Å—Ç–∞–≤–∫—É –Ω–∏–∂–µ", "–ø–æ–Ω–∏–∂–µ"]):
		prefs["rate"] = "low"
	if any(k in low for k in ["–ø–æ–∫–∞ –¥—É–º–∞–µ—Ç", "–¥—É–º–∞–µ—Ç", "–ø–æ–¥—É–º–∞—Ç—å"]):
		prefs["thinking"] = True
	return prefs


def _try_reply_deposit_rates(
	db: Database,
	tg_id: int,
	user_clean: str,
	today: date,
	force: bool = False,
	overrides: Optional[Dict[str, Any]] = None,
	prefer: Optional[str] = None,
) -> Optional[str]:
	lowq = user_clean.lower()
	# Broaden trigger: treat as deposit rates query if deposit or payout phrasing is present
	if not force and not any(k in lowq for k in ["–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç", "—Å—Ç–∞–≤–∫", "–µ–∂–µ–º–µ—Å—è—á", "–≤ –∫–æ–Ω—Ü–µ", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü"]):
		return None
	o = overrides or {}
	amt = _parse_amount_rub(user_clean) if _parse_amount_rub(user_clean) is not None else o.get("amount")
	pt = _parse_payout_type(user_clean) or o.get("payout_type")
	term = _parse_term_days(user_clean) or o.get("term_days")
	# If neither provided, ask a single clarification
	if amt is None and pt is None and term is None:
		return (
			"–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: –≤—ã–ø–ª–∞—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ 1) –µ–∂–µ–º–µ—Å—è—á–Ω–æ –∏–ª–∏ 2) –≤ –∫–æ–Ω—Ü–µ —Å—Ä–æ–∫–∞, —Å—É–º–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 300‚ÄØ000 ‚ÇΩ), –∏ —Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)."
		)
	# Query rates
	when = None  # do not filter by dates to allow rows with NULL effective_from/to
	# Channel filter for ¬´–ú–æ–π –î–æ–º¬ª
	channel = None
	if "–º–æ–π –¥–æ–º" in lowq or "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫" in lowq or "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –±–∞–Ω–∫" in lowq:
		channel = "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ë–∞–Ω–∫"
	# Detect currency from query (‚ÇΩ/$/‚Ç¨/¬•), else no filter
	curr = _detect_currency(user_clean) or o.get("currency") or "RUB"
	if channel is None:
		channel = "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ë–∞–Ω–∫"
	rows = db.product_rates_query(pt, term, amt, when, channel=channel, currency=curr, source_like=None)
	if not rows:
		# Fallback loosen filters stepwise
		if term is not None:
			rows = db.product_rates_query(pt, None, amt, when, channel=channel, currency=curr, source_like=None)
		if not rows and amt is not None:
			rows = db.product_rates_query(pt, term, None, when, channel=channel, currency=curr, source_like=None)
		if not rows and amt is not None and term is not None:
			rows = db.product_rates_query(pt, None, None, when, channel=channel, currency=curr, source_like=None)
	if not rows:
		return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å—Ç–∞–≤–∫–∞—Ö –ø–æ –≤–∫–ª–∞–¥–∞–º –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫."
	# If result set is big and user didn't ask to 'show all', ask for clarifications to avoid overly long answer
	if "–ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ" not in lowq:
		too_many = len(rows) > 10
		missing_keys = []
		if pt is None:
			missing_keys.append("–≤—ã–ø–ª–∞—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (–µ–∂–µ–º–µ—Å—è—á–Ω–æ/–≤ –∫–æ–Ω—Ü–µ)")
		if amt is None:
			missing_keys.append("–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è —Å—É–º–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1‚ÄØ000‚ÄØ000 ‚ÇΩ)")
		if term is None:
			missing_keys.append("—Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)")
		# If many rows or missing key filters ‚Äî ask 1 clarifying message
		if too_many or missing_keys:
			# Build compact hints from data
			terms = sorted({int(r.get("term_days", 0)) for r in rows if r.get("term_days")})
			plans = sorted({(r.get("plan_name") or "").strip() for r in rows if (r.get("plan_name") or "").strip()})
			curropts = sorted({(r.get("currency") or "").strip() for r in rows if (r.get("currency") or "").strip()})
			term_hint = ("; —Å—Ä–æ–∫–∏: " + ", ".join(map(str, terms[:10])) + (" ‚Ä¶" if len(terms) > 10 else "")) if terms else ""
			plan_hint = ("; —Ç–∞—Ä–∏—Ñ—ã: " + ", ".join(plans[:5]) + (" ‚Ä¶" if len(plans) > 5 else "")) if plans else ""
			cur_hint = ("; –≤–∞–ª—é—Ç—ã: " + ", ".join(curropts)) if curropts else ""
			need = "; ".join(missing_keys) if missing_keys else "—É—Ç–æ—á–Ω–∏—Ç–µ —Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)"
			return (
				"–ß—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –±—ã–ª –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –∏ –∫–æ—Ä–æ—Ç–∫–∏–º, —É—Ç–æ—á–Ω–∏—Ç–µ: " + need + ".\n"
				f"–ú–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: '–µ–∂–µ–º–µ—Å—è—á–Ω–æ, 1‚ÄØ000‚ÄØ000 ‚ÇΩ, 181 –¥–Ω–µ–π, {curr}'.\n–ü–æ–¥—Å–∫–∞–∑–∫–∏"+term_hint+plan_hint+cur_hint+".\n"
				"–ù–∞–ø–∏—à–∏—Ç–µ '–ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ', –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫."
			)
	# Group by payout_type -> term_days -> amount bucket
	def _fmt_amount(val: Optional[float], curr: Optional[str]) -> str:
		if val is None:
			return ""
		try:
			num = f"{float(val):,.0f}".replace(",", " ")
		except Exception:
			num = str(val)
		if (curr or "").upper() == "RUB":
			return f"{num} ‚ÇΩ"
		return f"{num} {curr or ''}".strip()
	def _bucket(r: Dict[str, Any]) -> str:
		amin = float(r.get("amount_min") or 0)
		amax = r.get("amount_max")
		curr = (r.get("currency") or "").upper() or None
		if amax is None:
			return f"–æ—Ç {_fmt_amount(amin, curr)}"
		return f"{_fmt_amount(amin, curr)}‚Äì{_fmt_amount(float(amax), curr)}"
	# Build concise header
	header_parts: List[str] = ["–ü–æ–¥–±–æ—Ä –≤–∫–ª–∞–¥–æ–≤"]
	if term is not None:
		header_parts.append(f"–Ω–∞ —Å—Ä–æ–∫ {term} –¥–Ω–µ–π")
	if pt is not None:
		header_parts.append("—Å –µ–∂–µ–º–µ—Å—è—á–Ω–æ–π –≤—ã–ø–ª–∞—Ç–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤" if pt == "monthly" else "—Å –≤—ã–ø–ª–∞—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ —Å—Ä–æ–∫–∞")
	if amt is not None:
		header_parts.append(f"–Ω–∞ —Å—É–º–º—É {_fmt_amount(amt, curr)}")
	header = " ".join(header_parts) + ":"
	# Helper to normalize percent from row
	def _rate_pct_of(r: Dict[str, Any]) -> float:
		val = float(r.get("rate_percent") or 0)
		return (val * 100.0) if val <= 1.0 else val
	# detect prefs for conversational tone
	prefs_local = _detect_preferences(user_clean)
	# Sort by term, then rate according to preference (default desc), then amount_min
	if prefer == "low":
		r_sorted = sorted(rows, key=lambda r: (int(r.get("term_days", 0)), (_rate_pct_of(r)), float(r.get("amount_min") or 0)))
	else:
		r_sorted = sorted(rows, key=lambda r: (int(r.get("term_days", 0)), -(_rate_pct_of(r)), float(r.get("amount_min") or 0)))
	lines = [header]
	# Cap the number of listed items to keep the message concise
	MAX_OUTPUT_LINES = 5
	count = 0
	for r in r_sorted:
		term_r = int(r.get("term_days", 0))
		if term is not None and term_r != term:
			continue
		plan = (r.get("plan_name") or "").strip()
		if not plan:
			continue
		# Note: –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω—É–º–µ—Ä—É–µ–º –∏ –Ω–µ –≤—ã–≤–æ–¥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
		lines.append(f"- {plan}: {_rate_pct_of(r):.1f}%")
		count += 1
		if count >= MAX_OUTPUT_LINES:
			break
	# Recommend top tariffs separately (numbered)
	if prefer == "low":
		top = sorted([r for r in r_sorted if (term is None or int(r.get("term_days", 0)) == term)], key=lambda r: _rate_pct_of(r))[:2]
	else:
		top = sorted([r for r in r_sorted if (term is None or int(r.get("term_days", 0)) == term)], key=lambda r: _rate_pct_of(r), reverse=True)[:2]
	reco = ""
	if top:
		reco_lines = []
		for i, t in enumerate(top, start=1):
			pname = (t.get("plan_name") or "").strip()
			reco_lines.append(f"{i}) {pname}: {_rate_pct_of(t):.1f}%")
		reco = "\n–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ (–ø–æ —Å—Ç–∞–≤–∫–µ):\n" + "\n".join(reco_lines)
	# Coaching block (conversational)
	coach_lines: List[str] = []
	if prefs_local.get("rate") == "high":
		coach_lines.append("–ï—Å–ª–∏ –≤–∞–∂–Ω–∞ —Å—Ç–∞–≤–∫–∞ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –≤—ã—à–µ; –∫—Ä–∞—Ç–∫–æ –æ–±—Ä–∏—Å—É–π—Ç–µ –≤—ã–≥–æ–¥—É.")
	if prefs_local.get("thinking"):
		coach_lines.append("–§—Ä–∞–∑–∞: '–ü–æ–Ω–∏–º–∞—é, –º–æ–∂–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —É—Å–ª–æ–≤–∏—è —Å–µ–≥–æ–¥–Ω—è, –∞ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω—è—Ç—å –ø–æ—Å–ª–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è ‚Äî —É–¥–æ–±–Ω–µ–µ –∫–ª–∏–µ–Ω—Ç—É'.")
	coach = ("\n–ß—Ç–æ —Å–∫–∞–∑–∞—Ç—å –∫–ª–∏–µ–Ω—Ç—É:\n" + "\n".join(["- " + s for s in coach_lines])) if coach_lines else ""
	# Actions single line
	actions = "\n–î–µ–π—Å—Ç–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞: –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∞—Ä–∏—Ñ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏ –ø–æ–º–æ–≥–∏—Ç–µ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥ –∫–ª–∏–µ–Ω—Ç—É"
	return "\n".join(lines) + ("\n" + reco if reco else "") + coach + actions 


# ------------------------ Generative coaching helper ------------------------

def _generate_coaching_reply(client: OpenAI, user_text: str, given_text: str) -> str:
	"""Generate a short, conversational coaching block without inventing numbers.
	- Keep it actionable (3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤) and product-agnostic.
	- Do NOT include links or numeric rates; refer to given_text abstractly.
	"""
	system = (
		"–¢—ã ‚Äî AI BDM‚Äë–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–µ, –∂–∏–≤—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥. "
		"–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ü–∏—Ñ—Ä—ã. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏. –¢–æ–Ω ‚Äî –¥–µ–ª–æ–≤–æ–π, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã."
	)
	messages = [
		{"role": "system", "content": system},
		{"role": "user", "content": f"–í–æ–ø—Ä–æ—Å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞:\n{user_text}\n\n–î–∞–Ω–æ (—É—Å–ª–æ–≤–∏—è/—Å—Ç–∞–≤–∫–∏, –±–µ–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è):\n{given_text}\n\n–°—Ñ–æ—Ä–º–∏—Ä—É–π 3‚Äì5 –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏ –∫–æ—Ä–æ—Ç–∫–∏–π —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥."},
	]
	settings = get_settings()
	text = _chat_completion_with_fallback(
		client=client,
		model=settings.assistant_model,
		messages=messages,
		temperature=0.5,
		max_tokens=700,
	)
	return text


# ------------------------ System prompt builder ------------------------

def _build_system_prompt(agent_name: str, stats_line: str, group_line: str, notes_preview: str) -> str:
	system = (
		"–¢—ã ‚Äî AI BDM-–∫–æ—É—á –¥–ª—è –≤—ã–µ–∑–¥–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –±–∞–Ω–∫–∞. –†–∞–∑—Ä–µ—à–µ–Ω–æ —Ç–æ–ª—å–∫–æ: –ø—Ä–æ–¥—É–∫—Ç—ã [–ö–ù, –ö–ö, –î–ö, –ö–°–ü, –ò–ö, –ò–ó–ü, –ù–°, –í–∫–ª–∞–¥, –ö–ù –∫ –ó–ü], –∫—Ä–æ—Å—Å-–ø—Ä–æ–¥–∞–∂–∏, –ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã/–ø–ª–∞–Ω—ã/—Ä–µ–π—Ç–∏–Ω–≥, –∫–æ—É—á–∏–Ω–≥, —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—Å—Ç—Ä–µ—á–∏. –í–Ω–µ —Ç–µ–º–∞—Ç–∏–∫–∏ ‚Äî –º—è–≥–∫–æ –≤–µ—Ä–Ω–∏ –∫ —Ä–∞–±–æ—á–∏–º –≤–æ–ø—Ä–æ—Å–∞–º. –ü–î–Ω –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–π –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π.\n\n"
		"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: 1) FACTS (–ë–î: —Ç–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã ‚Äî —Å—Ç–∞–≤–∫–∏/–ª–∏–º–∏—Ç—ã/–∫–æ–º–∏—Å—Å–∏–∏/—Å—Ä–æ–∫–∏/—Å—É–º–º—ã) ‚Üí 2) SOURCES (RAG: –ø—Ä–∞–≤–∏–ª–∞/–∏—Å–∫–ª—é—á–µ–Ω–∏—è). –õ—é–±–∞—è —Ü–∏—Ñ—Ä–∞ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ—Ç—Å—è [F#], –ø—Ä–∞–≤–∏–ª–∞ ‚Äî [S#]. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫'.\n\n"
		"–î–æ–ø—É—Å—Ç–∏ 1 —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –±–µ–∑ –Ω–µ–≥–æ –Ω–µ–ª—å–∑—è –¥–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —É–∫–∞–∑–∞–Ω—ã –≤–∞–ª—é—Ç–∞/–∫–∞–Ω–∞–ª/—Å—Ä–æ–∫/—Å—É–º–º–∞/—Ç–∏–ø –≤—ã–ø–ª–∞—Ç—ã).\n\n"
		"–°—Ç–∏–ª—å: –∫—Ä–∞—Ç–∫–æ, –¥–µ–ª–æ–≤–æ, –±–µ–∑ –≤–æ–¥—ã, –±–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ –∏ —ç–º–æ–¥–∑–∏.\n\n"
		"–§–æ—Ä–º–∞—Ç:\n1) –°–≤–æ–¥–∫–∞ (1‚Äì2 —Å—Ç—Ä–æ–∫–∏)\n2) –¶–∏—Ñ—Ä—ã FACTS (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ, —Å [F#])\n3) –ö–ª—é—á–µ–≤—ã–µ —É—Å–ª–æ–≤–∏—è (–∏–∑ —Ç–æ–≥–æ –∂–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Å [S#])\n4) –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–¥–∞–∂–µ (3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤, –±–µ–∑ —á–∏—Å–µ–ª)\n5) –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥/—É—Ç–æ—á–Ω–µ–Ω–∏–µ (1 –≤–æ–ø—Ä–æ—Å –º–∞–∫—Å–∏–º—É–º)\n"
	)
	return system



def _parse_period(user_text: str, today: date) -> Tuple[date, date, str]:
	low = user_text.lower()
	# Explicit date range dd.mm.yyyy - dd.mm.yyyy
	import re
	m = re.search(r"(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})\s*[‚Äì\-]\s*(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})", low)
	if m:
		def to_d(s: str) -> date:
			parts = re.split(r"[.\-/]", s)
			day, mon, year = map(int, parts)
			return date(year, mon, day)
		start = to_d(m.group(1))
		end = to_d(m.group(2))
		return start, end, f"–ø–µ—Ä–∏–æ–¥ {m.group(1)}‚Äì{m.group(2)}"
	if "—Å–µ–≥–æ–¥–Ω—è" in low:
		return today, today, "—Å–µ–≥–æ–¥–Ω—è"
	if "–≤—á–µ—Ä–∞" in low:
		y = today - timedelta(days=1)
		return y, y, "–≤—á–µ—Ä–∞"
	if "–Ω–µ–¥–µ–ª" in low:
		start_week = today - timedelta(days=today.weekday())
		return start_week, today, "—Ç–µ–∫—É—â–∞—è –Ω–µ–¥–µ–ª—è"
	if "–º–µ—Å—è—Ü" in low:
		start_month = today.replace(day=1)
		return start_month, today, "—Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü"
	# default: today
	return today, today, "—Å–µ–≥–æ–¥–Ω—è"



def _is_stats_request(text: str) -> bool:
	low = text.lower()
	# Ignore internal auto-summary prompts
	if "[auto_summary]" in low:
		return False
	keys = ["—Å—Ç–∞—Ç–∏—Å—Ç", "–∏—Ç–æ–≥", "–ª–∏–¥–µ—Ä", "—Ä–µ–π—Ç–∏–Ω–≥", "—Å–∫–æ–ª—å–∫–æ —Å–¥–µ–ª–∞–ª", "–ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"]
	return any(k in low for k in keys)



def _is_off_topic(text: str) -> bool:
	low = text.lower().strip()
	if low.isdigit():
		return False
	product_words = ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑","–∫—Ä–µ–¥–∏—Ç","–∫–∞—Ä—Ç–∞","–∏–ø–æ—Ç–µ–∫","—Å—Ç—Ä–∞—Ö–æ–≤","–∑–∞—Ä–ø–ª–∞—Ç","–Ω–∞–∫–æ–ø–∏—Ç–µ–ª"]
	if any(w in low for w in product_words):
		return False
	off_cues = [
		"–ø–æ–≥–æ–¥–∞", "–∞–Ω–µ–∫–¥–æ—Ç", "–∫–∏–Ω–æ", "–∏–≥—Ä–∞", "—Ç—Ä–∞–º–ø", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç",
	]
	return any(w in low for w in off_cues)



def _format_stats_reply(period_label: str, total: int, by_product: Dict[str, int], leaders: List[Dict[str, Any]]) -> str:
	items = [(p, c) for p, c in by_product.items() if c > 0]
	items.sort(key=lambda x: x[1], reverse=True)
	products_str = ", ".join([f"{p}:{c}" for p, c in items]) if items else "–Ω–µ—Ç"
	leaders_str = ", ".join([f"{r['agent_name']}:{r['total']}" for r in leaders[:3]]) if leaders else "–Ω–µ—Ç"
	settings = get_settings()
	if settings.emoji_stats:
		return (
			f"1. –ü–µ—Ä–∏–æ–¥: {period_label} üìÖ\n"
			f"2. –ò—Ç–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total} üéØ\n"
			f"3. –ü–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º: {products_str} üìä\n"
			f"4. –õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã: {leaders_str} üèÖ"
		)
	return (
		f"1. –ü–µ—Ä–∏–æ–¥: {period_label}\n"
		f"2. –ò—Ç–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total}\n"
		f"3. –ü–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º: {products_str}\n"
		f"4. –õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã: {leaders_str}"
	)



def _redirect_reply() -> str:
	return (
		"–≠—Ç–æ –≤–Ω–µ —Ä–∞–±–æ—á–∏—Ö —Ç–µ–º. –í–µ—Ä–Ω—ë–º—Å—è –∫ –¥–µ–ª—É: –ø—Ä–æ–¥—É–∫—Ç—ã, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.\n"
		"1. –†–∞–∑–±–æ—Ä –≤—Å—Ç—Ä–µ—á–∏\n2. –¶–µ–ª—å –Ω–∞ –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—é\n3. –ü–ª–∞–Ω –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"
	)



def _normalize_bullets(text: str) -> str:
	"""Ensure that numbered bullets '1)', '2)' (and legacy '1.') start on new lines.
	- Inserts a newline before any occurrence of '<digits>) ' or '<digits>. ' not already at line start.
	- Ensures '-' sub-bullets start on a new line.
	- If a numbered item has exactly one immediate sub-bullet, inline it on the same line without '-'.
	- Collapses extra spaces around newlines.
	"""
	if not text:
		return ""
	# Normalize newlines first
	normalized = text.replace("\r\n", "\n").replace("\r", "\n")
	# Insert newline before N) or N. where N=1..99 if not already at start of line
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\)\s)", "\n", normalized)
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\.\s)", "\n", normalized)
	# Insert newline before hyphen bullets "- " when not at line start
	normalized = re.sub(r"(?<!^)\s+(?=-\s)", "\n", normalized)
	# Trim trailing spaces per line
	lines = [ln.strip() for ln in normalized.split("\n") if ln.strip()]
	# Inline single sub-bullet into its parent numbered item
	result: List[str] = []
	i = 0
	while i < len(lines):
		line = lines[i]
		m_num = re.match(r"^(\d{1,2}\))\s+(.*)", line)
		if not m_num:
			# also support legacy '1.' pattern
			m_num = re.match(r"^(\d{1,2})\.\s+(.*)", line)
			if m_num:
				# convert to 1) style
				num = m_num.group(1) + ")"
				main = m_num.group(2)
				# check next line for single sub-bullet
				if i + 1 < len(lines):
					m_sub = re.match(r"^-\s+(.*)", lines[i + 1])
					# ensure only one sub-bullet (next next starts new numbered or end)
					is_single = False
					if m_sub:
						if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
							is_single = True
					if m_sub and is_single:
						result.append(f"{num} {main} {m_sub.group(1)}")
						i += 2
						continue
					else:
						result.append(f"{num} {main}")
						i += 1
						continue
			else:
				# not a numbered line, keep as-is
				result.append(line)
				i += 1
				continue
		# Here we have 1) pattern already
		num = m_num.group(1)
		main = m_num.group(2)
		if i + 1 < len(lines):
			m_sub2 = re.match(r"^-\s+(.*)", lines[i + 1])
			is_single2 = False
			if m_sub2:
				if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
					is_single2 = True
			if m_sub2 and is_single2:
				result.append(f"{num} {main} {m_sub2.group(1)}")
				i += 2
				continue
		# default: just append numbered line
		result.append(f"{num} {main}")
		i += 1
	# Join back
	return "\n".join(result).strip()



def _strip_md_emphasis(text: str) -> str:
	"""Remove markdown emphasis like **bold** or *italic* without touching bullets."""
	if not text:
		return ""
	import re as _re
	# **bold** -> bold
	text = _re.sub(r"\*\*(.*?)\*\*", r"\1", text)
	# *italic* -> italic (avoid converting list markers)
	text = _re.sub(r"(?<!^)\*(?!\s)([^*]+?)\*(?!\S)", r"\1", text, flags=_re.MULTILINE)
	# Remove stray double-asterisks
	return text.replace("**", "")


CURRENCY_HINTS = {
	"RUB": ["—Ä—É–±", "‚ÇΩ", "rub", "–≤ —Ä—É–±", "—Ä—É–±."],
	"USD": ["usd", "$", "–¥–æ–ª–ª–∞—Ä"],
	"EUR": ["eur", "‚Ç¨", "–µ–≤—Ä–æ"],
	"CNY": ["cny", "¬•", "—é–∞–Ω", "—é–∞–Ω–∏"],
}


def _detect_currency(query: str) -> Optional[str]:
	low = query.lower().replace('\u00a0',' ').replace(' ', '')
	for code, keys in CURRENCY_HINTS.items():
		for k in keys:
			kk = k.replace(' ', '')
			if kk in low:
				return code
	return None


def _vector_top_chunks(db: Database, product: Optional[str], currency: Optional[str], query: str, k: int = 5) -> list[Dict[str, Any]]:
	"""Vector search via RPC if embeddings are present. Returns rows with content/currency/product_code/distance."""
	try:
		# simple embedding using same model as ingestion
		client = OpenAI(api_key=get_settings().openai_api_key)
		e = client.embeddings.create(model="text-embedding-3-small", input=query)
		emb = e.data[0].embedding
		res = db.client.rpc(
			"match_rag_chunks",
			{"product": product, "currency_in": currency, "query_embedding": emb, "match_count": k},
		).execute()
		rows = getattr(res, "data", []) or []
		return rows
	except Exception:
		return []


def _rag_snippets(db: Database, product_hint: Optional[str], limit: int = 5) -> List[Dict[str, str]]:
	"""Fetch top RAG snippets from rag_docs by product_code or keyword in title/content.
	Simple heuristic until pgvector is added: filter by product_code, else keyword in content.
	"""
	try:
		if product_hint:
			res = db.client.table("rag_docs").select("id,url,title,content,product_code").ilike("product_code", product_hint).order("fetched_at", desc=True).limit(limit).execute()
			rows = getattr(res, "data", []) or []
			if rows:
				return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows]
		# fallback: recent docs
		res2 = db.client.table("rag_docs").select("id,url,title,content,product_code").order("fetched_at", desc=True).limit(limit).execute()
		rows2 = getattr(res2, "data", []) or []
		return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows2]
	except Exception:
		return []



def _extract_rate_lines(text: str) -> list[str]:
	"""Extract lines with percent patterns to guide the model toward concrete rates."""
	lines = [l.strip() for l in text.split('\n') if l.strip()]
	res: list[str] = []
	import re as _re
	for l in lines:
		if _re.search(r"\d{1,2}(?:[.,]\d)?\s*%", l):
			res.append(l)
	return res[:6]



def _rag_top_chunks(db: Database, product_hint: Optional[str], query: str, limit_docs: int = 3, limit_chunks: int = 5) -> Tuple[List[str], Dict[str, Any]]:
	"""Pick top chunks for product with currency awareness.
	Returns (texts, meta) where meta contains currencies set and extracted rate lines and sources list.
	Order: vector search (product/currency filters) ‚Üí keyword fallback ‚Üí doc content fallback.
	"""
	currency = _detect_currency(query)
	# 1) vector search with strict product filter (no cross-product fallback)
	vec_rows = []  # disabled chunk vector search since rag_chunks removed
	if vec_rows:
		texts = [r.get("content", "") for r in vec_rows if r.get("content")]
		currs = {r.get("currency") for r in vec_rows if r.get("currency")}
		rate_lines: list[str] = []
		for t in texts:
			for rl in _extract_rate_lines(t):
				rate_lines.append(rl)
		meta = {"currencies": list({c for c in currs if c}), "rates": rate_lines[:10], "via": "vector", "sources": []}
		return texts, meta
	# Fallback: use recent rag_docs by product
	base_docs: List[Dict[str, Any]] = []
	if product_hint:
		base_docs = db.select_rag_docs_by_product(product_hint, limit=limit_docs)
	# If no docs by product, return empty context
	if not base_docs:
		return [], {"currencies": [], "rates": [], "via": "docs", "sources": []}
	# Use first N docs' content snippets
	texts = [(d.get("content") or "")[:1200] for d in base_docs][:limit_chunks]
	rate_lines: list[str] = []
	for t in texts:
		for rl in _extract_rate_lines(t):
			rate_lines.append(rl)
	meta = {
		"currencies": [],
		"rates": rate_lines[:10],
		"via": "docs",
		"sources": [{"title": d.get("title", ""), "url": d.get("url", ""), "id": d.get("id")} for d in base_docs],
	}
	return texts, meta



def _build_fact_label(product: str, f: Dict[str, Any]) -> str:
	if product == "–í–∫–ª–∞–¥":
		plan = (f.get("plan_name") or "").strip()
		td = f.get("term_days")
		amin = f.get("amount_min")
		amax = f.get("amount_max")
		label = plan
		if td:
			label += f", {int(td)} –¥–Ω"
		if amin is not None:
			range_str = ""
			try:
				lo = f"{float(amin):,.0f}".replace(","," ")
			except Exception:
				lo = str(amin)
			if amax is not None:
				try:
					hi = f"{float(amax):,.0f}".replace(","," ")
				except Exception:
					hi = str(amax)
				range_str = f"{lo}‚Äì{hi}"
			else:
				range_str = f"–æ—Ç {lo}"
			label += f", {range_str}"
		return label.strip(", ")
	# generic products
	key = (f.get("fact_key") or "").strip()
	td = f.get("term_days")
	chn = (f.get("channel") or "").strip()
	parts: List[str] = [key]
	if td:
		parts.append(f"{int(td)} –¥–Ω")
	if chn:
		parts.append(chn)
	return ", ".join([p for p in parts if p])


def _build_fact_value(product: str, f: Dict[str, Any]) -> str:
	if product == "–í–∫–ª–∞–¥":
		val = f.get("rate_percent")
		curr = (f.get("currency") or "").upper()
		if val is None:
			return ""
		v100 = float(val) if float(val) > 1 else float(val) * 100
		return f"{v100:.1f}% {curr}".strip()
	# generic products
	if f.get("value_numeric") is not None:
		try:
			vn = float(f["value_numeric"]) 
			if f.get("fact_key","" ).endswith("pct"):
				return f"{vn:.1f}%"
			return f"{vn:.0f}"
		except Exception:
			pass
	return (f.get("value_text") or "").strip()


def _map_natural_term(text: str) -> Optional[int]:
    low = text.lower()
    if any(k in low for k in ["–≥–æ–¥", "12 –º–µ—Å", "12 –º–µ—Å—è—Ü–µ–≤", "–Ω–∞ –≥–æ–¥"]):
        return 367
    if any(k in low for k in ["–ø–æ–ª–≥–æ–¥–∞", "6 –º–µ—Å", "6 –º–µ—Å—è—Ü–µ–≤", "–Ω–∞ –ø–æ–ª –≥–æ–¥–∞", "–Ω–∞ –ø–æ–ª–≥–æ–¥–∞"]):
        return 181
    if any(k in low for k in ["–∫–≤–∞—Ä—Ç–∞–ª", "3 –º–µ—Å", "3 –º–µ—Å—è—Ü–∞"]):
        return 91
    return None


def _synthesize_from_playbook(rows: List[Dict[str, Any]], user_text: str) -> str:
    if not rows:
        return "–í –ø–ª–µ–π–±—É–∫–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å."
    facts = []
    for r in rows[:6]:
        snip = (r.get("snippet") or "").replace("**", "").strip()
        anchor = r.get("anchor") or f"¬ß{r.get('ord')}"
        facts.append(f"- {snip} [#doc:–ü–ª–µ–π–±—É–∫ {anchor}]")
    takeaway = "\n–í—ã–≤–æ–¥: –ø–æ–¥—Å—Ç—Ä–æ–π—Ç–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –ø–æ–¥ –∫–ª–∏–µ–Ω—Ç–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥."
    return "\n".join(facts) + takeaway

def try_reply_financial(db: Database, product: str, slots: Dict[str, Any]) -> Optional[str]:
    # Switch to Playbook (docs/doc_passages)
    query = slots.get("query") or ""
    rows = db.search_playbook(query or product or "–ü–ª–µ–π–±—É–∫", limit=8)
    return _synthesize_from_playbook(rows, query)


def validate_numbers(answer: str, has_facts: bool) -> str:
	res: List[str] = []
	for ln in answer.splitlines():
		has_num = bool(re.search(r"\d", ln))
		has_ref = bool(re.search(r"\[(?:F|S)\d+\]", ln))
		if has_num and not has_ref:
			continue
		if has_num and has_facts and re.search(r"\[S\d+\]", ln):
			continue
		res.append(ln)
	return "\n".join(res).strip()


def _is_deposit_rates_intent(text: str) -> bool:
	low = text.lower()
	if any(k in low for k in ["—Å—Ç–∞–≤–∫", "%", "–ø—Ä–æ—Ü–µ–Ω—Ç", "–µ–∂–µ–º–µ—Å—è—á", "–≤ –∫–æ–Ω—Ü–µ", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü"]):
		return True
	try:
		if _parse_amount_rub(text) is not None:
			return True
	except Exception:
		pass
	try:
		if _parse_term_days(text) is not None:
			return True
	except Exception:
		pass
	return False


def dc_detect_sales_stage(text: str) -> Optional[str]:
    low = text.lower()
    # objections
    if any(k in low for k in ["–≤–æ–∑—Ä–∞–∂", "–¥–æ—Ä–æ–≥–æ", "–¥—Ä—É–≥–æ–º –±–∞–Ω–∫–µ", "—Å–æ–º–Ω–µ–≤–∞", "—Ä–∏—Å–∫", "–Ω–µ —Ö–æ—á—É", "–Ω–µ –±—É–¥—É"]):
        return "–≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è"
    # closing / finishing
    if any(k in low for k in ["–æ—Ñ–æ—Ä–º", "–∑–∞–∫—Ä—ã", "–ø–æ–¥—ã—Ç–æ–∂", "–∏—Ç–æ–≥", "–¥–∞–ª—å—à–µ", "–≥–æ—Ç–æ–≤", "–ø–µ—Ä–µ–π–¥—ë–º", "–ø–µ—Ä–µ—Ö–æ–¥–∏–º"]):
        return "–∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ"
    # default to sales/discovery
    if any(k in low for k in ["–∫–∞–∫ –ø—Ä–æ–¥–∞—Ç—å", "–ø—Ä–æ–¥–∞—Ç—å", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü", "–æ–±—ä—è—Å–Ω–∏", "—Ä–∞—Å—Å–∫–∞–∂–∏", "—É—Å–ª–æ–≤–∏—è", "–∫–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å"]):
        return "–ø—Ä–æ–¥–∞–∂–∞"
    return None

def _filter_docs_by_stage(docs: List[Dict[str, Any]], stage: str) -> List[Dict[str, Any]]:
    stage_low = (stage or "").lower()
    out = []
    for d in docs:
        title = (d.get("title") or "").lower()
        url = (d.get("url") or "").lower()
        if stage_low and (stage_low in title or f"#{stage_low}/" in url):
            out.append(d)
    return out


def get_assistant_reply(db: Database, tg_id: int, agent_name: str, user_stats: Dict[str, Any], group_month_ranking: List[Dict[str, Any]], user_message: str) -> str:
	settings = get_settings()
	client = OpenAI(api_key=settings.openai_api_key)

	user_clean = sanitize_text_assistant_output(user_message)
	# Natural term mapping into slots (best-effort)
	try:
		mapped = _map_natural_term(user_clean)
		if mapped:
			prev = db.get_slots(tg_id)
			db.set_slots(tg_id, term_days=mapped, product_code=prev.get("product_code") or "–í–∫–ª–∞–¥")
	except Exception:
		pass
	# Detect internal auto-summary prompts early to adjust flow
	auto_summary = "[auto_summary]" in user_clean.lower()
	today = date.today()
	# Ensure period vars are always defined
	start, end, period_label = today, today, "—Å–µ–≥–æ–¥–Ω—è"
	try:
		start, end, period_label = _parse_period(user_clean, today)
	except Exception:
		pass

	# Phrase handlers with high precedence
	low = user_clean.lower()
	if re.search(r"—á—Ç–æ\s+–∑–Ω–∞—á–∏—Ç\s+(–æ–¥–∏–Ω\s+)?—Å–ª–µ–¥—É—é—â(–∏–π|–µ–≥–æ)\s+—à–∞–≥", low):
		ans = "–≠—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: –≤—ã–±—Ä–∞—Ç—å —Ç–∞—Ä–∏—Ñ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ë–∞–Ω–∫–µ, –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —Å—É–º–º—É/—Å—Ä–æ–∫ –∏ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥. –ó–∞–π–º—ë—Ç 3‚Äì5 –º–∏–Ω—É—Ç. –ì–æ—Ç–æ–≤—ã? 1) –î–∞  2) –ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∞  3) –°—Ä–∞–≤–Ω–∏—Ç—å —Å –ù–°"
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
		return ans
	if re.search(r"–æ—Ñ–æ—Ä–º–ª—è(–µ–º|—Ç—å)\s+–æ–Ω–ª–∞–π–Ω|–≥–æ—Ç–æ–≤(–∞|)\s+–æ—Ñ–æ—Ä–º|–æ—Ç–∫—Ä(–æ–π|—ã—Ç—å|—ã–≤–∞—é)\s+–≤–∫–ª–∞–¥", low):
		ans = "–û—Ç–ª–∏—á–Ω–æ, –æ—Ñ–æ—Ä–º–∏–º –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ë–∞–Ω–∫–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: –ø–∞—Å–ø–æ—Ä—Ç –ø–æ–¥ —Ä—É–∫–æ–π, –¥–æ—Å—Ç—É–ø –≤ –ò–ë. –û—Ç–∫—Ä–æ—é 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –Ω–∞ –≤—ã–±–æ—Ä ‚Äî –∫–∞–∫–æ–π –ø—Ä–µ–¥–ø–æ—á—Ç—ë—Ç–µ? 1) –ï–∂–µ–º–µ—Å—è—á–Ω–æ  2) –í –∫–æ–Ω—Ü–µ  3) –ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∞"
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
		return ans

	# Always answer from Playbook (docs/doc_passages)
	if ("–≤–∫–ª–∞–¥" in user_clean.lower() or "–¥–µ–ø–æ–∑–∏—Ç" in user_clean.lower()) and not _is_deposit_rates_intent(user_clean):
		# In-branch phrase handlers to keep interactivity
		low2 = user_clean.lower()
		if re.search(r"—á—Ç–æ\s+–∑–Ω–∞—á–∏—Ç\s+(–æ–¥–∏–Ω\s+)?—Å–ª–µ–¥—É—é—â(–∏–π|–µ–≥–æ)\s+—à–∞–≥", low2):
			ans = "–≠—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: –≤—ã–±—Ä–∞—Ç—å —Ç–∞—Ä–∏—Ñ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ë–∞–Ω–∫–µ, –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —Å—É–º–º—É/—Å—Ä–æ–∫ –∏ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥. –ó–∞–π–º—ë—Ç 3‚Äì5 –º–∏–Ω—É—Ç. –ì–æ—Ç–æ–≤—ã? 1) –î–∞  2) –ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∞  3) –°—Ä–∞–≤–Ω–∏—Ç—å —Å –ù–°"
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
			return ans
		if re.search(r"–æ—Ñ–æ—Ä–º–ª—è(–µ–º|—Ç—å)\s+–æ–Ω–ª–∞–π–Ω|–≥–æ—Ç–æ–≤(–∞|)\s+–æ—Ñ–æ—Ä–º|–æ—Ç–∫—Ä(–æ–π|—ã—Ç—å|—ã–≤–∞—é)\s+–≤–∫–ª–∞–¥", low2):
			ans = "–û—Ç–ª–∏—á–Ω–æ, –æ—Ñ–æ—Ä–º–∏–º –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ë–∞–Ω–∫–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: –ø–∞—Å–ø–æ—Ä—Ç –ø–æ–¥ —Ä—É–∫–æ–π, –¥–æ—Å—Ç—É–ø –≤ –ò–ë. –û—Ç–∫—Ä–æ—é 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –Ω–∞ –≤—ã–±–æ—Ä ‚Äî –∫–∞–∫–æ–π –ø—Ä–µ–¥–ø–æ—á—Ç—ë—Ç–µ? 1) –ï–∂–µ–º–µ—Å—è—á–Ω–æ  2) –í –∫–æ–Ω—Ü–µ  3) –ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∞"
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
			return ans
		rows = db.search_playbook(user_clean, product="–ü–ª–µ–π–±—É–∫", limit=8)
		ans = _synthesize_from_playbook(rows, user_clean)
		ans = sanitize_text_assistant_output(ans)
		ans = _normalize_bullets(ans)
		ans = _strip_md_emphasis(ans)
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
		return ans

	# Period data only for explicit stats requests to avoid unnecessary deps
	if _is_stats_request(user_clean):
		period_stats = db.stats_period(tg_id, start, end)
		plan_info = db.compute_plan_breakdown(tg_id, today)
		# previous period for comparison
		prev_start = start - (end - start) - timedelta(days=1)
		prev_end = start - timedelta(days=1)
		prev_stats = db.stats_period(tg_id, prev_start, prev_end)
		group_rank = db.group_ranking_period(start, end)
		reply = _format_stats_reply(period_label, int(period_stats.get("total", 0)), period_stats.get("by_product", {}), group_rank)
		reply_clean = sanitize_text(reply)
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", reply_clean, off_topic=False)
		return reply_clean
	# Defaults when not a stats request
	period_stats = {"total": 0, "by_product": {}}
	plan_info = {"plan_day": 0, "plan_week": 0, "plan_month": 0, "rr_month": 0}
	group_rank = []

	# Merge slots and deterministic branches only for normal chats (not auto-summary)
	if not auto_summary:
		# Merge slots: load existing and update from current message
		slots = db.get_slots(tg_id)
		# Extract from current message
		curr = _detect_currency(user_clean) or slots.get("currency")
		amt = _parse_amount_rub(user_clean) if _parse_amount_rub(user_clean) is not None else slots.get("amount")
		pt = _parse_payout_type(user_clean) or slots.get("payout_type")
		term = _parse_term_days_smart(user_clean, db, product="–í–∫–ª–∞–¥") or slots.get("term_days")
		# expanded product intents
		PRODUCT_INTENTS = {
			"–í–∫–ª–∞–¥": ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑–∏—Ç","–¥–µ–ø–æ–∑"],
			"–ö–ù": ["–∫–Ω","–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á","–Ω–∞–ª–∏—á–Ω","–ø–æ—Ç—Ä–µ–±"],
			"–ö–ö": ["–∫–∫","–∫—Ä–µ–¥–∏—Ç–Ω –∫–∞—Ä—Ç","–∫—Ä–µ–¥–∏—Ç–Ω–∞—è –∫–∞—Ä—Ç"],
			"–î–ö": ["–¥–∫","–¥–µ–±–µ—Ç–æ–≤","–¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç"],
			"–ö–°–ü": ["–∫—Å–ø","—Å—Ç—Ä–∞—Ö–æ–≤","–∫–æ—Ä–æ–±–æ—á–Ω"],
			"–ò–ö": ["–∏–ø–æ—Ç–µ–∫","–∏–ø–æ—Ç–µ—á–Ω"],
			"–ò–ó–ü": ["–∏–∑–ø","–∑–∞—Ä–ø–ª–∞—Ç–Ω –ø—Ä–æ–µ–∫—Ç","–∑–∞—Ä–ø–ª–∞—Ç"],
			"–ù–°": ["–Ω–∞–∫–æ–ø–∏—Ç","–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–π —Å—á–µ—Ç","–Ω–∞–∫–æ–ø–∏—Ç —Å—á–µ—Ç"],
			"–ö–ù –∫ –ó–ü": ["–∫–Ω –∫ –∑–ø","–∫—Ä–µ–¥–∏—Ç –∫ –∑–∞—Ä–ø–ª–∞—Ç"],
		}
		product_hint = slots.get("product_code")
		for code, keys in PRODUCT_INTENTS.items():
			if any(k in user_clean.lower() for k in keys):
				product_hint = code
				break
		try:
			db.set_slots(tg_id, product_code=product_hint, currency=curr, amount=amt, payout_type=pt, term_days=term)
		except Exception:
			pass
		# For deposits: if question is not about rates/—Ü–∏—Ñ—Ä—ã ‚Äî reply from RAG docs immediately (no numbers)
		if product_hint == "–í–∫–ª–∞–¥" and not _is_deposit_rates_intent(user_clean):
			# In-branch phrase handlers to avoid RAG loop
			low3 = user_clean.lower()
			if re.search(r"—á—Ç–æ\s+–∑–Ω–∞—á–∏—Ç\s+(–æ–¥–∏–Ω\s+)?—Å–ª–µ–¥—É—é—â(–∏–π|–µ–≥–æ)\s+—à–∞–≥", low3):
				ans = "–≠—Ç–æ –∫–æ—Ä–æ—Ç–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: –≤—ã–±—Ä–∞—Ç—å —Ç–∞—Ä–∏—Ñ –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ë–∞–Ω–∫–µ, –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —Å—É–º–º—É/—Å—Ä–æ–∫ –∏ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥. –ó–∞–π–º—ë—Ç 3‚Äì5 –º–∏–Ω—É—Ç. –ì–æ—Ç–æ–≤—ã? 1) –î–∞  2) –ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∞  3) –°—Ä–∞–≤–Ω–∏—Ç—å —Å –ù–°"
				db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
				db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
				return ans
			if re.search(r"–æ—Ñ–æ—Ä–º–ª—è(–µ–º|—Ç—å)\s+–æ–Ω–ª–∞–π–Ω|–≥–æ—Ç–æ–≤(–∞|)\s+–æ—Ñ–æ—Ä–º|–æ—Ç–∫—Ä(–æ–π|—ã—Ç—å|—ã–≤–∞—é)\s+–≤–∫–ª–∞–¥", low3):
				ans = "–û—Ç–ª–∏—á–Ω–æ, –æ—Ñ–æ—Ä–º–∏–º –≤ –ò–Ω—Ç–µ—Ä–Ω–µ—Ç‚Äë–ë–∞–Ω–∫–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: –ø–∞—Å–ø–æ—Ä—Ç –ø–æ–¥ —Ä—É–∫–æ–π, –¥–æ—Å—Ç—É–ø –≤ –ò–ë. –û—Ç–∫—Ä–æ—é 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –Ω–∞ –≤—ã–±–æ—Ä ‚Äî –∫–∞–∫–æ–π –ø—Ä–µ–¥–ø–æ—á—Ç—ë—Ç–µ? 1) –ï–∂–µ–º–µ—Å—è—á–Ω–æ  2) –í –∫–æ–Ω—Ü–µ  3) –ü–æ–∫–∞–∑–∞—Ç—å –æ–±–∞"
				db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
				db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
				return ans
			# Fast-path: "–∫–∞–∫–∏–µ —É—Å–ª–æ–≤–∏—è" -> go to FACTS with defaults instead of repeating RAG
			if re.search(r"\b(–∫–∞–∫–∏–µ|—Ç–∞–∫ –∫–∞–∫–∏–µ)\s+—É—Å–ª–æ–≤–∏—è\b", user_clean.lower()):
				def_slots = {"currency": "RUB", "channel": "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ë–∞–Ω–∫"}
				ans = try_reply_financial(db, "–í–∫–ª–∞–¥", def_slots) or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫."
				ans = sanitize_text_assistant_output(ans)
				ans = _normalize_bullets(ans)
				ans = _strip_md_emphasis(ans)
				ans = validate_numbers(ans, has_facts=True)
				if tg_id != 195830791:
					ans = re.sub(r"\s?\[(?:F|S)\d+\]", "", ans)
				db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
				db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
				return ans
			rows = db.search_playbook(user_clean, product="–ü–ª–µ–π–±—É–∫", limit=8)
			ans = _synthesize_from_playbook(rows, user_clean)
			ans = sanitize_text_assistant_output(ans)
			ans = _normalize_bullets(ans)
			ans = _strip_md_emphasis(ans)
			ans = validate_numbers(ans, has_facts=False)
			if tg_id != 195830791:
				ans = re.sub(r"\s?\[S\d+\]", "", ans)
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
			return ans
		# Try unified financial responder first
		fin = try_reply_financial(db, product_hint or "–ü–ª–µ–π–±—É–∫", {"query": user_clean}) if product_hint else None
		if fin:
			ans = sanitize_text_assistant_output(fin)
			ans = _normalize_bullets(ans)
			ans = _strip_md_emphasis(ans)
			# Hide [F#]/[S#] for all except tg id == 195830791
			if tg_id != 195830791:
				ans = re.sub(r"\s?\[(?:F|S)\d+\]", "", ans)
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
			return ans

	# Notes only from employee for context
	notes = db.list_notes_period(tg_id, start, end, limit=3)
	notes_preview = "\n".join([f"{i+1}. {n['content_sanitized']}" for i, n in enumerate(notes)]) if notes else "‚Äî"

	# Compose messages for model
	stats_line = (
		f"{period_label}: –≤—Å–µ–≥–æ {period_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {period_stats['by_product']}; "
		f"–ø–ª–∞–Ω –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—è/–º–µ—Å—è—Ü {plan_info['plan_day']}/{plan_info['plan_week']}/{plan_info['plan_month']}; RR {plan_info['rr_month']}"
	)
	prev_line = f"–ü—Ä–µ–¥—ã–¥—É—â–∏–π –ø–µ—Ä–∏–æ–¥: –≤—Å–µ–≥–æ {prev_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {prev_stats['by_product']}"
	best = ", ".join([f"{r['agent_name']}:{r['total']} ]" for r in group_rank[:2]]) if group_rank else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
	group_line = f"–õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã –∑–∞ {period_label}: {best}"
	# RAG context (silent for user): do not set product_hint/guards for auto-summary
	product_hint = None
	if not auto_summary:
		for k in ["–ö–ù","–∫–Ω","–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á","–Ω–∞–ª–∏—á–Ω","–Ω–∞–ª–∏—á","–ø–æ—Ç—Ä–µ–±","–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—Å–∫","–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π","–ø–æ—Ç—Ä","–Ω–∞–ª–∏—á–Ω—ã–µ"]:
			if k in user_clean.lower():
				product_hint = "–ö–ù"
				break
		# deposits
		if not product_hint:
			for k in ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑–∏—Ç","–¥–µ–ø–æ–∑" ]:
				if k in user_clean.lower():
					product_hint = "–í–∫–ª–∞–¥"
					break
	rag_texts, rag_meta = _rag_top_chunks(db, product_hint, user_clean, limit_docs=3, limit_chunks=5)
	ctx_text = "\n\n".join(rag_texts) if rag_texts else ""
	# Clarify currency/guards only in normal chats
	if not auto_summary:
		# Clarify currency if ambiguous
		detected_curr = _detect_currency(user_clean)
		if (not detected_curr) and rag_meta.get("currencies") and len(rag_meta["currencies"]) > 1:
			question = "–£—Ç–æ—á–Ω–∏—Ç–µ –≤–∞–ª—é—Ç—É: 1) RUB (‚ÇΩ), 2) USD ($), 3) EUR (‚Ç¨), 4) CNY (¬•)?"
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", question, off_topic=False)
			return question
		# Guard for KN/Deposit numeric citations
		if product_hint in ("–ö–ù","–í–∫–ª–∞–¥") and not rag_meta.get("rates"):
			msg = "–£—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–∞–ª—é—Ç–∞/—Ç–∞—Ä–∏—Ñ/–∫–∞–Ω–∞–ª), –ø—Ä–∏—à–ª—é —Ü–∏—Ñ—Ä—ã."
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", msg, off_topic=False)
			return msg
	try:
		db.log(tg_id, "rag_ctx", {"count": len(rag_texts) if rag_texts else 0, "previews": [t[:200] for t in (rag_texts or [])], "currencies": rag_meta.get("currencies", []), "via": rag_meta.get("via")})
	except Exception:
		pass

	messages: List[Dict[str, str]] = []
	messages.append({"role": "system", "content": _build_system_prompt(agent_name, stats_line + "; " + prev_line, group_line, notes_preview)})
	# Inject structured FACTS and SOURCES for downstream citation [F#]/[S#], except for auto-summary prompts
	auto_summary = "[auto_summary]" in user_clean.lower()
	if not auto_summary:
		# Compute day/week/month metrics to align FACTS with auto-summary
		try:
			# today
			today_total, _today_by = db._sum_attempts_query(tg_id, today, today)
			p_day = int(plan_info.get('plan_day', 0))
			c_day = (today_total * 100 / p_day) if p_day > 0 else 0
			# meetings and penetration for today
			m_day = db.meets_period_count(tg_id, today, today)
			linked_day = db.attempts_linked_period_count(tg_id, today, today)
			pen_day = (linked_day * 100 / m_day) if m_day > 0 else 0
			facts_lines: List[str] = []
			facts_lines.append(f"F1: –°–µ–≥–æ–¥–Ω—è —Ñ–∞–∫—Ç ‚Äî {today_total}")
			facts_lines.append(f"F2: –°–µ–≥–æ–¥–Ω—è –ø–ª–∞–Ω ‚Äî {p_day}")
			facts_lines.append(f"F3: –°–µ–≥–æ–¥–Ω—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_day))}")
			facts_lines.append(f"F4: –°–µ–≥–æ–¥–Ω—è –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ, % ‚Äî {int(round(pen_day))}")
			# minimal week/month anchors
			start_week = today - timedelta(days=today.weekday())
			week_total, _ = db._sum_attempts_query(tg_id, start_week, today)
			p_week = int(plan_info.get('plan_week', 0))
			c_week = (week_total * 100 / p_week) if p_week > 0 else 0
			facts_lines.append(f"F5: –ù–µ–¥–µ–ª—è —Ñ–∞–∫—Ç ‚Äî {week_total}")
			facts_lines.append(f"F6: –ù–µ–¥–µ–ª—è –ø–ª–∞–Ω ‚Äî {p_week}")
			facts_lines.append(f"F7: –ù–µ–¥–µ–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_week))}")
			start_month = today.replace(day=1)
			month_total, _ = db._sum_attempts_query(tg_id, start_month, today)
			p_month = int(plan_info.get('plan_month', 0))
			c_month = (month_total * 100 / p_month) if p_month > 0 else 0
			facts_lines.append(f"F8: –ú–µ—Å—è—Ü —Ñ–∞–∫—Ç ‚Äî {month_total}")
			facts_lines.append(f"F9: –ú–µ—Å—è—Ü –ø–ª–∞–Ω ‚Äî {p_month}")
			facts_lines.append(f"F10: –ú–µ—Å—è—Ü –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_month))}")
			facts_lines.append(f"F11: RR –º–µ—Å—è—Ü–∞ (–ø—Ä–æ–≥–Ω–æ–∑ —Ñ–∞–∫—Ç–∞) ‚Äî {int(plan_info.get('rr_month', 0))}")
			sources_lines: List[str] = []
			for i, s in enumerate((rag_meta.get("sources") or [])[:5], start=1):
				title = (s.get("title") or "–ò—Å—Ç–æ—á–Ω–∏–∫").strip()
				url = (s.get("url") or "").strip()
				sources_lines.append(f"S{i}: {title} ‚Äî {url}")
			fs_block = ("FACTS:\n" + "\n".join(facts_lines)) + ("\n\n" + ("SOURCES:\n" + "\n".join(sources_lines)) if sources_lines else "")
			messages.append({"role": "system", "content": fs_block})
		except Exception:
			pass
	if ctx_text:
		# Inject rate lines separately to anchor exact numbers; instruct to cite with [S#]
		rate_block = "\n".join(rag_meta.get("rates", []) or [])
		add = "–°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏; –≤ –æ—Ç–≤–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏ [S#] –Ω–∞ SOURCES, URL –Ω–µ –≤—Å—Ç–∞–≤–ª—è–π –Ω–∞–ø—Ä—è–º—É—é):\n" + ctx_text
		if rate_block:
			add += "\n\n–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ —Å—Ç–∞–≤–∫–∞–º–∏ (–∏—Å–ø–æ–ª—å–∑—É–π –¥–æ—Å–ª–æ–≤–Ω–æ –∏ –≤—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –≤–∞–ª—é—Ç—É):\n" + rate_block
		messages.append({"role": "system", "content": add})
	# Keep broader chat history for context; include last 20
	history = db.get_assistant_messages(tg_id, limit=20)
	for m in history:
		messages.append({"role": m["role"], "content": m["content_sanitized"]})
	messages.append({"role": "user", "content": user_clean})

	answer = _chat_completion_with_fallback(
		client=client,
		model=settings.assistant_model,
		messages=messages,
		temperature=0.3,
		max_tokens=350,
	)
	answer_clean = sanitize_text_assistant_output(answer)
	answer_clean = _normalize_bullets(answer_clean)
	answer_clean = _strip_md_emphasis(answer_clean)
	# For non-deposit products (no FACTS), ensure no numbers are leaked
	if (not auto_summary) and (product_hint != "–í–∫–ª–∞–¥"):
		answer_clean = validate_numbers(answer_clean, has_facts=False)

	# Store
	db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
	db.add_assistant_message(tg_id, "assistant", answer_clean, off_topic=False)
	return answer_clean 


# ------------------------ Formatting helpers ------------------------

def _to_numbered(text: str) -> str:
	if not text:
		return ""
	# Normalize line breaks and split
	norm = text.replace("\r\n", "\n").replace("\r", "\n")
	raw_lines = [ln.strip() for ln in norm.split("\n") if ln.strip()]
	out: List[str] = []
	idx = 1
	for ln in raw_lines:
		# Keep section headers ending with ':' as-is
		if ln.endswith(":"):
			out.append(ln)
			continue
		# Strip common bullet markers
		clean = ln.lstrip("-‚Ä¢\t ")
		# Convert existing '1.' or '1)' to unified 'n)'
		m = re.match(r"^(\d{1,2})[)\.]+\s+(.*)", clean)
		if m:
			out.append(f"{idx}) {m.group(2)}")
		else:
			out.append(f"{idx}) {clean}")
		idx += 1
	return "\n".join(out) 


def _build_interactive_coach(product: str) -> str:
    """Return 4-step mini-script + a follow-up question with quick options.
    Kept generic, lightly tailored by product.
    """
    prod = (product or "").strip() or "–ø—Ä–æ–¥—É–∫—Ç—É"
    # Steps: probe ‚Üí value ‚Üí proof ‚Üí close, then question with options
    lines: List[str] = []
    if prod == "–í–∫–ª–∞–¥":
        lines.append("- –£—Ç–æ—á–Ω–µ–Ω–∏–µ: –∫–∞–∫–æ–π —Å—Ä–æ–∫/—Ç–∏–ø –≤—ã–ø–ª–∞—Ç—ã –≤–∞–∂–µ–Ω ‚Äî –µ–∂–µ–º–µ—Å—è—á–Ω–æ –∏–ª–∏ –≤ –∫–æ–Ω—Ü–µ?")
        lines.append("- –¶–µ–Ω–Ω–æ—Å—Ç—å: –º–æ–∂–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ—Ö–æ–¥, –ø—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ –≥—Ä–∞—Ñ–∏–∫—É ‚Äî —É–¥–æ–±–Ω–æ –≤–∏–¥–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
        lines.append("- –î–æ–≤–µ—Ä–∏–µ: –æ—Ñ–æ—Ä–º–ª—è–µ—Ç—Å—è –æ–Ω–ª–∞–π–Ω –∑–∞ 3‚Äì5 –º–∏–Ω—É—Ç, –≤–∫–ª–∞–¥ –∑–∞—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω –ê–°–í.")
        lines.append("- –ó–∞–∫—Ä—ã—Ç–∏–µ: –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ 1‚Äì2 —Ç–∞—Ä–∏—Ñ–∞ –Ω–∞ –≤—ã–±–æ—Ä, –ø–æ–¥ —Ü–µ–ª—å –∫–ª–∏–µ–Ω—Ç–∞.")
        q = "–ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ, –∫–∞–∫ —É–¥–æ–±–Ω–µ–µ: 1) –µ–∂–µ–º–µ—Å—è—á–Ω–æ 2) –≤ –∫–æ–Ω—Ü–µ 3) –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞?"
    elif prod == "–ö–ù":
        lines.append("- –£—Ç–æ—á–Ω–µ–Ω–∏–µ: —Å—É–º–º–∞/—Å—Ä–æ–∫/—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞ ‚Äî —á—Ç–æ –ø—Ä–∏–Ω—Ü–∏–ø–∏–∞–ª—å–Ω–æ?")
        lines.append("- –¶–µ–Ω–Ω–æ—Å—Ç—å: –ø–æ–¥ –≤–∞—à—É –∑–∞–¥–∞—á—É –ø—Ä–µ–¥–ª–æ–∂–∏–º –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π –ø–ª–∞—Ç—ë–∂ –∏ –±—ã—Å—Ç—Ä—ã–π –≤—ã–ø—É—Å–∫.")
        lines.append("- –î–æ–≤–µ—Ä–∏–µ: —Ä–µ—à–µ–Ω–∏–µ –æ–Ω–ª–∞–π–Ω, –±–µ–∑ –ª–∏—à–Ω–∏—Ö –≤–∏–∑–∏—Ç–æ–≤; –¥–æ–∫—É–º–µ–Ω—Ç—ã ‚Äî –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.")
        lines.append("- –ó–∞–∫—Ä—ã—Ç–∏–µ: 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –≤—ã–±–æ—Ä –∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥.")
        q = "–°–æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–π—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: 1) —Å—É–º–º–∞ 2) —Å—Ä–æ–∫ 3) –±–µ–∑ —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏ 4) –≤—Å—ë —Ä–∞–≤–Ω–æ ‚Äî –ø–æ–¥–æ–±—Ä–∞—Ç—å?"
    elif prod == "–ö–ö":
        lines.append("- –£—Ç–æ—á–Ω–µ–Ω–∏–µ: —á—Ç–æ –≤–∞–∂–Ω–µ–µ ‚Äî –ª—å–≥–æ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥, –∫—ç—à–±—ç–∫ –∏–ª–∏ –ª–∏–º–∏—Ç?")
        lines.append("- –¶–µ–Ω–Ω–æ—Å—Ç—å: –ø–æ–¥ –ø—Ä–∏–≤—ã—á–Ω—ã–µ —Ç—Ä–∞—Ç—ã ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫—ç—à–±—ç–∫/–ª—å–≥–æ—Ç–∞, –≤—ã–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ.")
        lines.append("- –î–æ–≤–µ—Ä–∏–µ: –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –æ–Ω–ª–∞–π–Ω –∑–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç, –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è ‚Äî –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.")
        lines.append("- –ó–∞–∫—Ä—ã—Ç–∏–µ: 1‚Äì2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Å–µ–π—á–∞—Å.")
        q = "–í—ã–±–µ—Ä–∏—Ç–µ –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—é: 1) –∫—ç—à–±—ç–∫ 2) –ª—å–≥–æ—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥ 3) –ª–∏–º–∏—Ç 4) –ø–æ–¥–æ–±—Ä–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏?"
    else:
        lines.append("- –£—Ç–æ—á–Ω–µ–Ω–∏–µ: —á—Ç–æ –≤–∞–∂–Ω–µ–µ –≤ –ø—Ä–æ–¥—É–∫—Ç–µ ‚Äî –ø—Ä–æ—Å—Ç–æ—Ç–∞, –¥–æ—Ö–æ–¥/—ç–∫–æ–Ω–æ–º–∏—è –∏–ª–∏ –≥–∏–±–∫–æ—Å—Ç—å?")
        lines.append("- –¶–µ–Ω–Ω–æ—Å—Ç—å: –ø—Ä–µ–¥–ª–æ–∂–∏–º 1‚Äì2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –ø–æ–¥ –≤–∞—à—É —Ü–µ–ª—å, –±–µ–∑ –ø–µ—Ä–µ–≥—Ä—É–∑–∞ –¥–µ—Ç–∞–ª—è–º–∏.")
        lines.append("- –î–æ–≤–µ—Ä–∏–µ: –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –æ–Ω–ª–∞–π–Ω, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ ‚Äî –≤ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–µ/–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.")
        lines.append("- –ó–∞–∫—Ä—ã—Ç–∏–µ: —Å–æ–≥–ª–∞—Å—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ –≤ 1 –∫–ª–∏–∫.")
        q = "–ß—Ç–æ –≤—ã–±–∏—Ä–∞–µ–º: 1) –ø—Ä–æ—Å—Ç–æ—Ç–∞ 2) –¥–æ—Ö–æ–¥/—ç–∫–æ–Ω–æ–º–∏—è 3) –≥–∏–±–∫–æ—Å—Ç—å 4) –ø–æ–∫–∞–∑–∞—Ç—å –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞?"
    return "\n".join(lines) + "\n" + q 