from __future__ import annotations

from typing import Any, Dict, List, Tuple
from datetime import date, datetime, timedelta

from openai import OpenAI

from .config import get_settings
from .pii import sanitize_text, sanitize_text_assistant_output
from .db import Database
import re
import os
from typing import Optional


ALLOWED_TOPICS_HINT = (
	"–±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã; –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏; —Å–∫—Ä–∏–ø—Ç—ã; —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞; —Ü–µ–ª–∏; –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π"
)


# ------------------------ Deposit rates helpers ------------------------

def _parse_amount_rub(text: str) -> Optional[float]:
	low = text.lower().replace("\u00a0", " ").replace("\u202f", " ")
	# 1) Explicit currency forms
	m = re.search(r"(\d[\d\s]{2,}(?:[.,]\d{1,2})?)\s*(?:—Ä—É–±|‚ÇΩ|rub)", low)
	if m:
		num = m.group(1).replace(" ", "").replace(",", ".")
		try:
			return float(num)
		except Exception:
			return None
	# 2) Word-based multipliers (–º–ª–Ω/—Ç—ã—Å) without currency
	m2 = re.search(r"(\d+(?:[.,]\d+)?)\s*(–º–ª–Ω|–º–∏–ª–ª–∏–æ–Ω|million|m|—Ç—ã—Å|—Ç—ã—Å—è—á|k)\b", low)
	if m2:
		val = float(m2.group(1).replace(",", "."))
		unit = m2.group(2)
		mult = 1.0
		if unit.startswith("–º–ª") or unit.startswith("mil") or unit == "m":
			mult = 1_000_000.0
		elif unit.startswith("—Ç—ã—Å") or unit.startswith("k"):
			mult = 1_000.0
		return val * mult
	# 3) Bare number likely representing RUB (>=5 digits)
	m3 = re.search(r"\b(\d[\d\s]{4,})\b", low)
	if m3:
		try:
			return float(m3.group(1).replace(" ", ""))
		except Exception:
			return None
	return None


def _parse_payout_type(text: str) -> Optional[str]:
	low = text.lower()
	if "–µ–∂–µ–º–µ—Å—è—á" in low or "–∫–∞–∂–¥—ã–π –º–µ—Å—è—Ü" in low:
		return "monthly"
	if "–≤ –∫–æ–Ω—Ü–µ" in low or "–ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏" in low or "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü" in low:
		return "end"
	return None


def _parse_term_days(text: str) -> Optional[int]:
	low = text.lower()
	# handle colloquial half-year
	if "–ø–æ–ª–≥–æ–¥–∞" in low or "–ø–æ–ª –≥–æ–¥–∞" in low or "–ø–æ–ª-–≥–æ–¥" in low:
		return 181
	# months mapping
	mon_map = {1:31,2:61,3:91,4:122,6:181,9:274,12:367,18:550,24:730,36:1100}
	m_mon = re.search(r"(\d+)\s*(?:–º–µ—Å|–º–µ—Å—è—Ü|–º–µ—Å—è—Ü–∞|–º–µ—Å—è—Ü–µ–≤)\b", low)
	if m_mon:
		mon = int(m_mon.group(1))
		return mon_map.get(mon, mon * 30)
	m_day = re.search(r"(\d+)\s*(?:–¥–Ω|–¥–Ω–µ–π|day|days)\b", low)
	if m_day:
		return int(m_day.group(1))
	# plain number that looks like days
	m_num = re.search(r"\b(\d{2,4})\b", low)
	if m_num:
		val = int(m_num.group(1))
		if 10 <= val <= 2000:
			return val
	return None


def _detect_preferences(text: str) -> Dict[str, Any]:
	low = text.lower()
	prefs: Dict[str, Any] = {}
	if any(k in low for k in ["—Å—Ç–∞–≤–∫ –ø–æ–≤—ã—à–µ", "—Å—Ç–∞–≤–∫ –ø–æ–≤—ã—à–µ", "—Å—Ç–∞–≤–∫–∞ –≤—ã—à–µ", "—Å—Ç–∞–≤–∫—É –≤—ã—à–µ", "–ø–æ–≤—ã—à–µ", "–≤—ã—à–µ", "–±–æ–ª—å—à–µ —Å—Ç–∞–≤–∫–∞", "—Å—Ç–∞–≤–∫–∞ –ø–æ–±–æ–ª—å—à–µ"]):
		prefs["rate"] = "high"
	elif any(k in low for k in ["–ø–æ–º–µ–Ω—å—à–µ", "–Ω–∏–∂–µ —Å—Ç–∞–≤–∫–∞", "—Å—Ç–∞–≤–∫–∞ –Ω–∏–∂–µ", "—Å—Ç–∞–≤–∫—É –Ω–∏–∂–µ", "–ø–æ–Ω–∏–∂–µ"]):
		prefs["rate"] = "low"
	if any(k in low for k in ["–ø–æ–∫–∞ –¥—É–º–∞–µ—Ç", "–¥—É–º–∞–µ—Ç", "–ø–æ–¥—É–º–∞—Ç—å"]):
		prefs["thinking"] = True
	return prefs


def _try_reply_deposit_rates(
	db: Database,
	tg_id: int,
	user_clean: str,
	today: date,
	force: bool = False,
	overrides: Optional[Dict[str, Any]] = None,
	prefer: Optional[str] = None,
) -> Optional[str]:
	lowq = user_clean.lower()
	# Broaden trigger: treat as deposit rates query if deposit or payout phrasing is present
	if not force and not any(k in lowq for k in ["–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç", "—Å—Ç–∞–≤–∫", "–µ–∂–µ–º–µ—Å—è—á", "–≤ –∫–æ–Ω—Ü–µ", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü"]):
		return None
	o = overrides or {}
	amt = _parse_amount_rub(user_clean) if _parse_amount_rub(user_clean) is not None else o.get("amount")
	pt = _parse_payout_type(user_clean) or o.get("payout_type")
	term = _parse_term_days(user_clean) or o.get("term_days")
	# If neither provided, ask a single clarification
	if amt is None and pt is None and term is None:
		return (
			"–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: –≤—ã–ø–ª–∞—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ 1) –µ–∂–µ–º–µ—Å—è—á–Ω–æ –∏–ª–∏ 2) –≤ –∫–æ–Ω—Ü–µ —Å—Ä–æ–∫–∞, —Å—É–º–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 300‚ÄØ000 ‚ÇΩ), –∏ —Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)."
		)
	# Query rates
	when = None  # do not filter by dates to allow rows with NULL effective_from/to
	# Channel filter for ¬´–ú–æ–π –î–æ–º¬ª
	channel = None
	if "–º–æ–π –¥–æ–º" in lowq or "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫" in lowq or "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –±–∞–Ω–∫" in lowq:
		channel = "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ë–∞–Ω–∫"
	# Detect currency from query (‚ÇΩ/$/‚Ç¨/¬•), else no filter
	curr = _detect_currency(user_clean) or o.get("currency")
	rows = db.product_rates_query(pt, term, amt, when, channel=channel, currency=curr, source_like=None)
	if not rows:
		# Fallback loosen filters stepwise
		if term is not None:
			rows = db.product_rates_query(pt, None, amt, when, channel=channel, currency=curr, source_like=None)
		if not rows:
			rows = db.product_rates_query(pt, None, amt, None)
	if not rows:
		return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å—Ç–∞–≤–∫–∞—Ö –ø–æ –≤–∫–ª–∞–¥–∞–º –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫."
	# If result set is big and user didn't ask to 'show all', ask for clarifications to avoid overly long answer
	if "–ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ" not in lowq:
		too_many = len(rows) > 30
		missing_keys = []
		if curr is None:
			missing_keys.append("–≤–∞–ª—é—Ç–∞ (RUB/USD/EUR/CNY)")
		if pt is None:
			missing_keys.append("–≤—ã–ø–ª–∞—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (–µ–∂–µ–º–µ—Å—è—á–Ω–æ/–≤ –∫–æ–Ω—Ü–µ)")
		if amt is None:
			missing_keys.append("–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è —Å—É–º–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1‚ÄØ000‚ÄØ000 ‚ÇΩ)")
		if term is None:
			missing_keys.append("—Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)")
		# If many rows or missing key filters ‚Äî ask 1 clarifying message
		if too_many or missing_keys:
			# Build compact hints from data
			terms = sorted({int(r.get("term_days", 0)) for r in rows if r.get("term_days")})
			plans = sorted({(r.get("plan_name") or "").strip() for r in rows if (r.get("plan_name") or "").strip()})
			curropts = sorted({(r.get("currency") or "").strip() for r in rows if (r.get("currency") or "").strip()})
			term_hint = ("; —Å—Ä–æ–∫–∏: " + ", ".join(map(str, terms[:10])) + (" ‚Ä¶" if len(terms) > 10 else "")) if terms else ""
			plan_hint = ("; —Ç–∞—Ä–∏—Ñ—ã: " + ", ".join(plans[:5]) + (" ‚Ä¶" if len(plans) > 5 else "")) if plans else ""
			cur_hint = ("; –≤–∞–ª—é—Ç—ã: " + ", ".join(curropts)) if curropts else ""
			need = "; ".join(missing_keys) if missing_keys else "—É—Ç–æ—á–Ω–∏—Ç–µ —Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)"
			return (
				"–ß—Ç–æ–±—ã –¥–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, —É—Ç–æ—á–Ω–∏—Ç–µ: " + need + ".\n"
				f"–ú–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: ‚Äò–µ–∂–µ–º–µ—Å—è—á–Ω–æ, 1‚ÄØ000‚ÄØ000 ‚ÇΩ, 181 –¥–Ω–µ–π, RUB‚Äô.\n–ü–æ–¥—Å–∫–∞–∑–∫–∏{term_hint}{plan_hint}{cur_hint}.\n"
				"–ù–∞–ø–∏—à–∏—Ç–µ ‚Äò–ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ‚Äô, –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥–ª–∏–Ω–Ω–æ)."
			)
	# Group by payout_type -> term_days -> amount bucket
	def _fmt_amount(val: Optional[float], curr: Optional[str]) -> str:
		if val is None:
			return ""
		try:
			num = f"{float(val):,.0f}".replace(",", " ")
		except Exception:
			num = str(val)
		if (curr or "").upper() == "RUB":
			return f"{num} ‚ÇΩ"
		return f"{num} {curr or ''}".strip()
	def _bucket(r: Dict[str, Any]) -> str:
		amin = float(r.get("amount_min") or 0)
		amax = r.get("amount_max")
		curr = (r.get("currency") or "").upper() or None
		if amax is None:
			return f"–æ—Ç {_fmt_amount(amin, curr)}"
		return f"{_fmt_amount(amin, curr)}‚Äì{_fmt_amount(float(amax), curr)}"
	# Build concise header
	header_parts: List[str] = ["–ü–æ–¥–±–æ—Ä –≤–∫–ª–∞–¥–æ–≤"]
	if term is not None:
		header_parts.append(f"–Ω–∞ —Å—Ä–æ–∫ {term} –¥–Ω–µ–π")
	if pt is not None:
		header_parts.append("—Å –µ–∂–µ–º–µ—Å—è—á–Ω–æ–π –≤—ã–ø–ª–∞—Ç–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤" if pt == "monthly" else "—Å –≤—ã–ø–ª–∞—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ —Å—Ä–æ–∫–∞")
	if amt is not None:
		header_parts.append(f"–Ω–∞ —Å—É–º–º—É {_fmt_amount(amt, curr)}")
	header = " ".join(header_parts) + ":"
	# Helper to normalize percent from row
	def _rate_pct_of(r: Dict[str, Any]) -> float:
		val = float(r.get("rate_percent") or 0)
		return (val * 100.0) if val <= 1.0 else val
	# detect prefs for conversational tone
	prefs_local = _detect_preferences(user_clean)
	# Sort by term, then rate according to preference (default desc), then amount_min
	if prefer == "low":
		r_sorted = sorted(rows, key=lambda r: (int(r.get("term_days", 0)), (_rate_pct_of(r)), float(r.get("amount_min") or 0)))
	else:
		r_sorted = sorted(rows, key=lambda r: (int(r.get("term_days", 0)), -(_rate_pct_of(r)), float(r.get("amount_min") or 0)))
	lines = [header]
	# Cap the number of listed items to keep the message concise
	MAX_OUTPUT_LINES = 20
	count = 0
	for r in r_sorted:
		term_r = int(r.get("term_days", 0))
		if term is not None and term_r != term:
			continue
		plan = (r.get("plan_name") or "").strip()
		if not plan:
			continue
		# Note: –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω—É–º–µ—Ä—É–µ–º –∏ –Ω–µ –≤—ã–≤–æ–¥–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
		lines.append(f"- {plan}: {_rate_pct_of(r):.1f}%")
		count += 1
		if count >= MAX_OUTPUT_LINES:
			break
	# Recommend top tariffs separately (numbered)
	if prefer == "low":
		top = sorted([r for r in r_sorted if (term is None or int(r.get("term_days", 0)) == term)], key=lambda r: _rate_pct_of(r))[:2]
	else:
		top = sorted([r for r in r_sorted if (term is None or int(r.get("term_days", 0)) == term)], key=lambda r: _rate_pct_of(r), reverse=True)[:2]
	reco = ""
	if top:
		reco_lines = []
		for i, t in enumerate(top, start=1):
			pname = (t.get("plan_name") or "").strip()
			reco_lines.append(f"{i}) {pname}: {_rate_pct_of(t):.1f}%")
		reco = "\n–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ (–ø–æ —Å—Ç–∞–≤–∫–µ):\n" + "\n".join(reco_lines)
	# Coaching block (conversational)
	coach_lines: List[str] = []
	if prefs_local.get("rate") == "high":
		coach_lines.append("–ï—Å–ª–∏ –≤–∞–∂–Ω–∞ —Å—Ç–∞–≤–∫–∞ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏—Ç–µ –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –≤—ã—à–µ; –∫—Ä–∞—Ç–∫–æ –æ–±—Ä–∏—Å—É–π—Ç–µ –≤—ã–≥–æ–¥—É.")
	if prefs_local.get("thinking"):
		coach_lines.append("–§—Ä–∞–∑–∞: ‚Äò–ü–æ–Ω–∏–º–∞—é, –º–æ–∂–Ω–æ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —É—Å–ª–æ–≤–∏—è —Å–µ–≥–æ–¥–Ω—è, –∞ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–∏–Ω—è—Ç—å –ø–æ—Å–ª–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è ‚Äî —É–¥–æ–±–Ω–µ–µ –∫–ª–∏–µ–Ω—Ç—É‚Äô.")
	coach = ("\n–ß—Ç–æ —Å–∫–∞–∑–∞—Ç—å –∫–ª–∏–µ–Ω—Ç—É:\n" + "\n".join(["- " + s for s in coach_lines])) if coach_lines else ""
	# Actions single line
	actions = "\n–î–µ–π—Å—Ç–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞: –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∞—Ä–∏—Ñ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏ –ø–æ–º–æ–≥–∏—Ç–µ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥ –∫–ª–∏–µ–Ω—Ç—É"
	return "\n".join(lines) + ("\n" + reco if reco else "") + coach + actions 


# ------------------------ Generative coaching helper ------------------------

def _generate_coaching_reply(client: OpenAI, user_text: str, given_text: str) -> str:
	"""Generate a short, conversational coaching block without inventing numbers.
	- Keep it actionable (3‚Äì5 –ø—É–Ω–∫—Ç–æ–≤) and product-agnostic.
	- Do NOT include links or numeric rates; refer to given_text abstractly.
	"""
	system = (
		"–¢—ã ‚Äî AI BDM‚Äë–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫. –î–∞–π –∫–æ—Ä–æ—Ç–∫–∏–µ, –∂–∏–≤—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º –∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥. "
		"–ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ü–∏—Ñ—Ä—ã. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏. –¢–æ–Ω ‚Äî –¥–µ–ª–æ–≤–æ–π, –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã."
	)
	messages = [
		{"role": "system", "content": system},
		{"role": "user", "content": f"–í–æ–ø—Ä–æ—Å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞:\n{user_text}\n\n–î–∞–Ω–æ (—É—Å–ª–æ–≤–∏—è/—Å—Ç–∞–≤–∫–∏, –±–µ–∑ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏—è):\n{given_text}\n\n–°—Ñ–æ—Ä–º–∏—Ä—É–π 3‚Äì5 –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏ –∫–æ—Ä–æ—Ç–∫–∏–π —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥."},
	]
	settings = get_settings()
	resp = client.chat.completions.create(
		model=settings.assistant_model,
		temperature=0.5,
		max_tokens=700,
		messages=messages,
	)
	return resp.choices[0].message.content or ""


# ------------------------ System prompt builder ------------------------

def _build_system_prompt(agent_name: str, stats_line: str, group_line: str, notes_preview: str) -> str:
	system = (
		"–¢—ã ‚Äî AI BDM –¥–ª—è –≤—ã–µ–∑–¥–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –±–∞–Ω–∫–∞. –Ø–≤–ª—è–µ—à—å—Å—è –º–∞—Å—Ç–µ—Ä–æ–º –ø—Ä–æ–¥–∞–∂: SPIN, –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π/–≤—ã–≥–æ–¥—ã, —Ä–∞–±–æ—Ç–∞ —Å –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∫—Ä–æ—Å—Å‚Äë –∏ –∞–ø—Å–µ–ª–ª –∏ –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤: –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –º–æ—Ç–∏–≤–∞—Ü–∏—è –∫ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—é —Ü–µ–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.\n\n"
		"–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–æ–≤: –î–∞–≤–∞–π –ø—Ä–∏–∫–ª–∞–¥–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏ —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥. –ü–æ–º–æ–≥–∞–π —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞–±–æ—Ç–µ: –ø—Ä–æ–¥—É–∫—Ç—ã, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Ü–µ–ª–∏, –∫–æ—É—á–∏–Ω–≥\n\n"
		"–°—Ç—Ä–æ–≥–æ –ø–æ —Ä–∞–º–∫–∞–º: –∫—Ä–∞—Ç–∫–∏–µ –≤—ã–≥–æ–¥—ã/—Å–∫—Ä–∏–ø—Ç—ã/–æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è; –ø–ª–∞–Ω—ã/—Ñ–∞–∫—Ç—ã; SMART‚Äë—à–∞–≥–∏. –û—Ç–≤–µ—Ç—ã –≤–Ω–µ —Ç–µ–º—ã, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ/–Ω–∞–ª–æ–≥–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –±–µ–∑ –±–∞–∑—ã –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ü–î–Ω ‚Äî –∑–∞–ø—Ä–µ—â–µ–Ω–æ.\n\n"
		"–°—Ç–∏–ª—å: –¥–µ–ª–æ–≤–æ–π, –±–µ–∑ –≤–æ–¥—ã. –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ø–∏—Å–∫–∏, –æ–¥–∏–Ω –ø—É–Ω–∫—Ç ‚Äî –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞. –ù–µ –±–æ–ª–µ–µ 1 —É—Ç–æ—á–Ω–µ–Ω–∏—è, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç.‚Ä®‚Ä®–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—à—å, –Ω–µ –ø—Ä–∏–≤–ª–µ–∫–∞—é—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –Ω–µ —É–ø—Ä–∞–≤–ª—è—é—Ç –∫–æ–º–∞–Ω–¥–æ–π, –∑–æ–Ω–∞ –∏—Ö –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ - –∫—Ä–æ—Å—Å-–ø—Ä–æ–¥–∞–∂–∞ –Ω–∞ –≤—Å—Ç—Ä–µ—á–µ —Å –∫–ª–∏–µ–Ω—Ç–æ–º.\n\n"
		"–¢—ã –±–µ—Ä–µ—à—å –¥–∞–Ω–Ω—ã–µ: –°–ù–ê–ß–ê–õ–ê FACTS (–ë–î: —Ç–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã ‚Äî —Å—Ç–∞–≤–∫–∏/–ª–∏–º–∏—Ç—ã/—Ç–∞—Ä–∏—Ñ—ã/—Å—Ä–æ–∫–∏/—Å—É–º–º—ã/–∫–æ–º–∏—Å—Å–∏–∏), –∑–∞—Ç–µ–º RAG (–ø—Ä–∞–≤–∏–ª–∞/–∏—Å–∫–ª—é—á–µ–Ω–∏—è/–æ–ø–∏—Å–∞–Ω–∏—è). –ï—Å–ª–∏ FACTS –Ω–µ—Ç ‚Äî –∏—â–∏ –≤ RAG; –µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî ¬´–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫¬ª.\n"
		"–°–ª–æ—Ç—ã (–ø—Ä–æ–¥—É–∫—Ç, –≤–∞–ª—é—Ç–∞, —Å—É–º–º–∞, —Å—Ä–æ–∫, —Ç–∏–ø –≤—ã–ø–ª–∞—Ç—ã/—Ç–∞—Ä–∏—Ñ–∞, –∫–∞–Ω–∞–ª) –ø–æ–º–Ω–∏ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ –¥–æ /cancel; —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ —Å–ª–æ—Ç–∞–º–∏ ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç FACTS.\n\n"
		"–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º/–≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö/–∞–≤—Ç–æ—Å–≤–æ–¥–∫–∞—Ö:\n"
		"1) –ö—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É—Å–ª–æ–≤–∏–π (–ø–æ —Å–ª–æ—Ç–∞–º/–≤–≤–æ–¥—É).\n"
		"2) –°–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ): ¬´- –ù–∞–∑–≤–∞–Ω–∏–µ: X%/Y ‚ÇΩ/Z —É—Å–ª. [F#]/[S#]¬ª.\n"
		"3) ¬´–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ: 1) ‚Ä¶ 2) ‚Ä¶¬ª ‚Äî –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏/–≤—ã–≥–æ–¥–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞.\n"
		"4) ¬´–î–µ–π—Å—Ç–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞: ‚Ä¶¬ª ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥.\n\n"
		"–ù–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π –æ—Ç–≤–µ—Ç: –µ—Å–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî —Å–ø—Ä–æ—Å–∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ (–≤–∞–ª—é—Ç–∞/—Å—É–º–º–∞/—Å—Ä–æ–∫/—Ç–∏–ø/–∫–∞–Ω–∞–ª).\n"
	)
	return system



def _parse_period(user_text: str, today: date) -> Tuple[date, date, str]:
	low = user_text.lower()
	# Explicit date range dd.mm.yyyy - dd.mm.yyyy
	import re
	m = re.search(r"(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})\s*[‚Äì\-]\s*(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})", low)
	if m:
		def to_d(s: str) -> date:
			parts = re.split(r"[.\-/]", s)
			day, mon, year = map(int, parts)
			return date(year, mon, day)
		start = to_d(m.group(1))
		end = to_d(m.group(2))
		return start, end, f"–ø–µ—Ä–∏–æ–¥ {m.group(1)}‚Äì{m.group(2)}"
	if "—Å–µ–≥–æ–¥–Ω—è" in low:
		return today, today, "—Å–µ–≥–æ–¥–Ω—è"
	if "–≤—á–µ—Ä–∞" in low:
		y = today - timedelta(days=1)
		return y, y, "–≤—á–µ—Ä–∞"
	if "–Ω–µ–¥–µ–ª" in low:
		start_week = today - timedelta(days=today.weekday())
		return start_week, today, "—Ç–µ–∫—É—â–∞—è –Ω–µ–¥–µ–ª—è"
	if "–º–µ—Å—è—Ü" in low:
		start_month = today.replace(day=1)
		return start_month, today, "—Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü"
	# default: today
	return today, today, "—Å–µ–≥–æ–¥–Ω—è"



def _is_stats_request(text: str) -> bool:
	low = text.lower()
	# Ignore internal auto-summary prompts
	if "[auto_summary]" in low:
		return False
	keys = ["—Å—Ç–∞—Ç–∏—Å—Ç", "–∏—Ç–æ–≥", "–ª–∏–¥–µ—Ä", "—Ä–µ–π—Ç–∏–Ω–≥", "—Å–∫–æ–ª—å–∫–æ —Å–¥–µ–ª–∞–ª", "–ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"]
	return any(k in low for k in keys)



def _is_off_topic(text: str) -> bool:
	low = text.lower().strip()
	# Numeric menu answer is allowed
	if low.isdigit():
		return False
	# Explicit off-topic cues ‚Üí True
	off_cues = [
		"–ø–æ–≥–æ–¥–∞", "—Ç—Ä–∞–º–ø", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "—Ä–µ–≥—Ä–µ—Å—Å–∏—è", "–∫–∏–Ω–æ", "–∏–≥—Ä–∞", "–∞–Ω–µ–∫–¥–æ—Ç",
		"–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∞–ª–ª–∞", "–ø—É–≥–∞—á–µ–≤–∞", "–ø—É–≥–∞—á—ë–≤–∞",
	]
	for c in off_cues:
		if c in low:
			return True
	# Default: treat as on-topic
	return False



def _format_stats_reply(period_label: str, total: int, by_product: Dict[str, int], leaders: List[Dict[str, Any]]) -> str:
	# Sort products by desc count, show all non-zero; if none, show "–Ω–µ—Ç"
	items = [(p, c) for p, c in by_product.items() if c > 0]
	items.sort(key=lambda x: x[1], reverse=True)
	products_str = ", ".join([f"{p}:{c}" for p, c in items]) if items else "–Ω–µ—Ç"
	leaders_str = ", ".join([f"{r['agent_name']}:{r['total']}]" for r in leaders[:3]]) if leaders else "–Ω–µ—Ç"
	return (
		f"1. –ü–µ—Ä–∏–æ–¥: {period_label} üìÖ\n"
		f"2. –ò—Ç–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total} üéØ\n"
		f"3. –ü–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º: {products_str} üìä\n"
		f"4. –õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã: {leaders_str} üèÖ"
	)



def _redirect_reply() -> str:
	return (
		"–≠—Ç–æ –≤–Ω–µ —Ä–∞–±–æ—á–∏—Ö —Ç–µ–º. –í–µ—Ä–Ω—ë–º—Å—è –∫ –¥–µ–ª—É: –ø—Ä–æ–¥—É–∫—Ç—ã, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.\n"
		"1. –†–∞–∑–±–æ—Ä –≤—Å—Ç—Ä–µ—á–∏\n2. –¶–µ–ª—å –Ω–∞ –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—é\n3. –ü–ª–∞–Ω –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"
	)



def _normalize_bullets(text: str) -> str:
	"""Ensure that numbered bullets '1)', '2)' (and legacy '1.') start on new lines.
	- Inserts a newline before any occurrence of '<digits>) ' or '<digits>. ' not already at line start.
	- Ensures '-' sub-bullets start on a new line.
	- If a numbered item has exactly one immediate sub-bullet, inline it on the same line without '-'.
	- Collapses extra spaces around newlines.
	"""
	if not text:
		return ""
	# Normalize newlines first
	normalized = text.replace("\r\n", "\n").replace("\r", "\n")
	# Insert newline before N) or N. where N=1..99 if not already at start of line
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\)\s)", "\n", normalized)
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\.\s)", "\n", normalized)
	# Insert newline before hyphen bullets "- " when not at line start
	normalized = re.sub(r"(?<!^)\s+(?=-\s)", "\n", normalized)
	# Trim trailing spaces per line
	lines = [ln.strip() for ln in normalized.split("\n") if ln.strip()]
	# Inline single sub-bullet into its parent numbered item
	result: List[str] = []
	i = 0
	while i < len(lines):
		line = lines[i]
		m_num = re.match(r"^(\d{1,2}\))\s+(.*)", line)
		if not m_num:
			# also support legacy '1.' pattern
			m_num = re.match(r"^(\d{1,2})\.\s+(.*)", line)
			if m_num:
				# convert to 1) style
				num = m_num.group(1) + ")"
				main = m_num.group(2)
				# check next line for single sub-bullet
				if i + 1 < len(lines):
					m_sub = re.match(r"^-\s+(.*)", lines[i + 1])
					# ensure only one sub-bullet (next next starts new numbered or end)
					is_single = False
					if m_sub:
						if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
							is_single = True
					if m_sub and is_single:
						result.append(f"{num} {main} {m_sub.group(1)}")
						i += 2
						continue
					else:
						result.append(f"{num} {main}")
						i += 1
						continue
			else:
				# not a numbered line, keep as-is
				result.append(line)
				i += 1
				continue
		# Here we have 1) pattern already
		num = m_num.group(1)
		main = m_num.group(2)
		if i + 1 < len(lines):
			m_sub2 = re.match(r"^-\s+(.*)", lines[i + 1])
			is_single2 = False
			if m_sub2:
				if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
					is_single2 = True
			if m_sub2 and is_single2:
				result.append(f"{num} {main} {m_sub2.group(1)}")
				i += 2
				continue
		# default: just append numbered line
		result.append(f"{num} {main}")
		i += 1
	# Join back
	return "\n".join(result).strip()



def _strip_md_emphasis(text: str) -> str:
	"""Remove markdown emphasis like **bold** or *italic* without touching bullets."""
	if not text:
		return ""
	import re as _re
	# **bold** -> bold
	text = _re.sub(r"\*\*(.*?)\*\*", r"\1", text)
	# *italic* -> italic (avoid converting list markers)
	text = _re.sub(r"(?<!^)\*(?!\s)([^*]+?)\*(?!\S)", r"\1", text, flags=_re.MULTILINE)
	# Remove stray double-asterisks
	return text.replace("**", "")


CURRENCY_HINTS = {
	"RUB": ["—Ä—É–±", "‚ÇΩ", "rub", "–≤ —Ä—É–±", "—Ä—É–±."],
	"USD": ["usd", "$", "–¥–æ–ª–ª–∞—Ä"],
	"EUR": ["eur", "‚Ç¨", "–µ–≤—Ä–æ"],
	"CNY": ["cny", "¬•", "—é–∞–Ω", "—é–∞–Ω–∏"],
}


def _detect_currency(query: str) -> Optional[str]:
	low = query.lower().replace('\u00a0',' ').replace(' ', '')
	for code, keys in CURRENCY_HINTS.items():
		for k in keys:
			kk = k.replace(' ', '')
			if kk in low:
				return code
	return None


def _vector_top_chunks(db: Database, product: Optional[str], currency: Optional[str], query: str, k: int = 5) -> list[Dict[str, Any]]:
	"""Vector search via RPC if embeddings are present. Returns rows with content/currency/product_code/distance."""
	try:
		# simple embedding using same model as ingestion
		client = OpenAI(api_key=get_settings().openai_api_key)
		e = client.embeddings.create(model="text-embedding-3-small", input=query)
		emb = e.data[0].embedding
		res = db.client.rpc(
			"match_rag_chunks",
			{"product": product, "currency_in": currency, "query_embedding": emb, "match_count": k},
		).execute()
		rows = getattr(res, "data", []) or []
		return rows
	except Exception:
		return []


def _rag_snippets(db: Database, product_hint: Optional[str], limit: int = 5) -> List[Dict[str, str]]:
	"""Fetch top RAG snippets from rag_docs by product_code or keyword in title/content.
	Simple heuristic until pgvector is added: filter by product_code, else keyword in content.
	"""
	try:
		if product_hint:
			res = db.client.table("rag_docs").select("id,url,title,content,product_code").ilike("product_code", product_hint).order("fetched_at", desc=True).limit(limit).execute()
			rows = getattr(res, "data", []) or []
			if rows:
				return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows]
		# fallback: recent docs
		res2 = db.client.table("rag_docs").select("id,url,title,content,product_code").order("fetched_at", desc=True).limit(limit).execute()
		rows2 = getattr(res2, "data", []) or []
		return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows2]
	except Exception:
		return []



def _extract_rate_lines(text: str) -> list[str]:
	"""Extract lines with percent patterns to guide the model toward concrete rates."""
	lines = [l.strip() for l in text.split('\n') if l.strip()]
	res: list[str] = []
	import re as _re
	for l in lines:
		if _re.search(r"\d{1,2}(?:[.,]\d)?\s*%", l):
			res.append(l)
	return res[:6]



def _rag_top_chunks(db: Database, product_hint: Optional[str], query: str, limit_docs: int = 3, limit_chunks: int = 5) -> Tuple[List[str], Dict[str, Any]]:
	"""Pick top chunks for product with currency awareness.
	Returns (texts, meta) where meta contains currencies set and extracted rate lines and sources list.
	Order: vector search (product/currency filters) ‚Üí keyword fallback ‚Üí doc content fallback.
	"""
	currency = _detect_currency(query)
	# 1) vector search with strict product filter (no cross-product fallback)
	vec_rows = _vector_top_chunks(db, product_hint, currency, query, k=limit_chunks)
	if vec_rows:
		texts = [r.get("content", "") for r in vec_rows if r.get("content")]
		currs = {r.get("currency") for r in vec_rows if r.get("currency")}
		rate_lines: list[str] = []
		for t in texts:
			for rl in _extract_rate_lines(t):
				rate_lines.append(rl)
		meta = {"currencies": list({c for c in currs if c}), "rates": rate_lines[:10], "via": "vector", "sources": []}
		return texts, meta
	# keywords from query: words >=3 chars
	words = [w for w in re.findall(r"[–ê-–Ø–∞-—èA-Za-z0-9%]+", query.lower()) if len(w) >= 3]
	ids: List[str] = []
	base_docs: List[Dict[str, Any]] = []
	try:
		base = _rag_snippets(db, product_hint, limit=limit_docs)
		base_docs = base
		ids = [r.get("id") for r in base if r.get("id")]
	except Exception:
		ids = []
		base_docs = []
	chunks: List[Dict[str, str]] = []
	if ids:
		try:
			res = db.client.table("rag_chunks").select("content, chunk_index, product_code, doc_id, currency").in_("doc_id", ids).limit(200).execute()
			rows = getattr(res, "data", []) or []
			for r in rows:
				chunks.append({"content": r.get("content",""), "chunk_index": int(r.get("chunk_index", 0)), "currency": r.get("currency")})
		except Exception:
			chunks = []
	if not chunks:
		# fallback: first 1200 of docs
		docs = _rag_snippets(db, product_hint, limit=limit_docs)
		texts = [d.get("content","")[:1200] for d in docs if d.get("content")] [:limit_chunks]
		meta = {"currencies": [], "rates": [], "via": "docs", "sources": [{"title": d.get("title",""), "url": d.get("url",""), "id": d.get("id")} for d in docs]}
		return texts, meta
	# score chunks
	scored: List[Tuple[int, Dict[str,str]]] = []
	for ch in chunks:
		text = ch["content"].lower()
		score = sum(text.count(w) for w in words) if words else 0
		# bonus for rate-like tokens to prioritize concrete terms
		if "%" in text:
			score += 5
		if "—Å—Ç–∞–≤–∫" in text:
			score += 3
		if "–≥–æ–¥–æ–≤—ã" in text:
			score += 2
		# prefer tariff/financial terms
		if "—Ç–∞—Ä–∏—Ñ" in text or "—Ñ–∏–Ω–∞–Ω—Å–æ–≤" in text:
			score += 3
		# currency agreement bonus/penalty
		if currency:
			if currency == "RUB" and ("—Ä—É–±" in text or "‚ÇΩ" in text):
				score += 4
			elif currency == "USD" and ("$" in text or "usd" in text or "–¥–æ–ª–ª–∞—Ä" in text):
				score += 4
			elif currency == "EUR" and ("‚Ç¨" in text or "eur" in text or "–µ–≤—Ä–æ" in text):
				score += 4
			elif currency == "CNY" and ("¬•" in text or "cny" in text or "—é–∞–Ω" in text):
				score += 4
			else:
				score -= 3
		scored.append((score, ch))
	scored.sort(key=lambda x: x[0], reverse=True)
	top_rows = [c for _, c in scored[:limit_chunks]]
	texts = [r["content"] for r in top_rows]
	currs = {r.get("currency") for r in top_rows if r.get("currency")}
	rate_lines: list[str] = []
	for t in texts:
		for rl in _extract_rate_lines(t):
			rate_lines.append(rl)
	# optional trace: store first 200 chars of each chosen chunk
	try:
		if texts:
			preview = [t[:200] for t in texts]
			# We cannot import db here; tracing is handled at call site in get_assistant_reply
			pass
	except Exception:
		pass
	meta = {"currencies": list({c for c in currs if c}), "rates": rate_lines[:10], "via": "keywords", "sources": [{"title": d.get("title",""), "url": d.get("url",""), "id": d.get("id")} for d in (base_docs or [])]}
	return texts, meta



def get_assistant_reply(db: Database, tg_id: int, agent_name: str, user_stats: Dict[str, Any], group_month_ranking: List[Dict[str, Any]], user_message: str) -> str:
	settings = get_settings()
	client = OpenAI(api_key=settings.openai_api_key)

	user_clean = sanitize_text_assistant_output(user_message)
	# Detect internal auto-summary prompts early to adjust flow
	auto_summary = "[auto_summary]" in user_clean.lower()
	today = date.today()
	start, end, period_label = _parse_period(user_clean, today)

	# Early off-topic block
	off_topic = _is_off_topic(user_clean)
	if off_topic:
		redirect = _redirect_reply()
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=True)
		db.add_assistant_message(tg_id, "assistant", sanitize_text(redirect), off_topic=False)
		return redirect

	# Period data + plans
	period_stats = db.stats_period(tg_id, start, end)
	plan_info = db.compute_plan_breakdown(tg_id, today)
	# previous period for comparison
	prev_start = start - (end - start) - timedelta(days=1)
	prev_end = start - timedelta(days=1)
	prev_stats = db.stats_period(tg_id, prev_start, prev_end)
	group_rank = db.group_ranking_period(start, end)

	# Direct stats reply with emojis if requested
	if _is_stats_request(user_clean):
		reply = _format_stats_reply(period_label, int(period_stats.get("total", 0)), period_stats.get("by_product", {}), group_rank)
		reply_clean = sanitize_text(reply)
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", reply_clean, off_topic=False)
		return reply_clean

	# Merge slots and deterministic branches only for normal chats (not auto-summary)
	if not auto_summary:
		# Merge slots: load existing and update from current message
		slots = db.get_slots(tg_id)
		# Extract from current message
		curr = _detect_currency(user_clean) or slots.get("currency")
		amt = _parse_amount_rub(user_clean) if _parse_amount_rub(user_clean) is not None else slots.get("amount")
		pt = _parse_payout_type(user_clean) or slots.get("payout_type")
		term = _parse_term_days(user_clean) or slots.get("term_days")
		product_hint = slots.get("product_code")
		# Detect product intent from current message and allow switching topic
		lowu = user_clean.lower()
		deposit_intent = any(k in lowu for k in ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑–∏—Ç","–¥–µ–ø–æ–∑"])
		credit_intent = any(k in lowu for k in ["–∫–Ω","–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á", "–Ω–∞–ª–∏—á–Ω", "–ø–æ—Ç—Ä–µ–±", "–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫", "–Ω–∞–ª–∏—á–Ω—ã–µ"])
		if deposit_intent:
			product_hint = "–í–∫–ª–∞–¥"
		elif credit_intent:
			product_hint = "–ö–ù"
		# Persist updated slots
		try:
			# Save even if only product intent changed
			db.set_slots(tg_id, product_code=product_hint, currency=curr, amount=amt, payout_type=pt, term_days=term)
		except Exception:
			pass
		# Deterministic branch: deposit rates from FACTS (product_rates)
		if product_hint == "–í–∫–ª–∞–¥":
			prefs = _detect_preferences(user_clean)
			prefer_rate = prefs.get("rate")
			over = {"currency": curr, "amount": amt, "payout_type": pt, "term_days": term}
			dep = _try_reply_deposit_rates(db, tg_id, user_clean, today, force=True, overrides=over, prefer=prefer_rate)
			if dep:
				ans = sanitize_text_assistant_output(dep)
				ans = _normalize_bullets(ans)
				# Add a conversational coaching addendum (second contour)
				coach = _generate_coaching_reply(client, user_clean, ans)
				coach_clean = sanitize_text_assistant_output(coach)
				coach_numbered = _to_numbered(coach_clean)
				final_reply = ans + ("\n\n" + coach_numbered if coach_numbered else "")
				# Remove markdown emphasis just in case
				final_reply = _strip_md_emphasis(final_reply)
				db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
				db.add_assistant_message(tg_id, "assistant", final_reply, off_topic=False)
				return final_reply

	# Notes only from employee for context
	notes = db.list_notes_period(tg_id, start, end, limit=3)
	notes_preview = "\n".join([f"{i+1}. {n['content_sanitized']}" for i, n in enumerate(notes)]) if notes else "‚Äî"

	# Compose messages for model
	stats_line = (
		f"{period_label}: –≤—Å–µ–≥–æ {period_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {period_stats['by_product']}; "
		f"–ø–ª–∞–Ω –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—è/–º–µ—Å—è—Ü {plan_info['plan_day']}/{plan_info['plan_week']}/{plan_info['plan_month']}; RR {plan_info['rr_month']}"
	)
	prev_line = f"–ü—Ä–µ–¥—ã–¥—É—â–∏–π –ø–µ—Ä–∏–æ–¥: –≤—Å–µ–≥–æ {prev_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {prev_stats['by_product']}"
	best = ", ".join([f"{r['agent_name']}:{r['total']} ]" for r in group_rank[:2]]) if group_rank else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
	group_line = f"–õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã –∑–∞ {period_label}: {best}"
	# RAG context (silent for user): do not set product_hint/guards for auto-summary
	product_hint = None
	if not auto_summary:
		for k in ["–ö–ù","–∫–Ω","–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á","–Ω–∞–ª–∏—á–Ω","–Ω–∞–ª–∏—á","–ø–æ—Ç—Ä–µ–±","–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫","–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π","–ø–æ—Ç—Ä","–Ω–∞–ª–∏—á–Ω—ã–µ"]:
			if k in user_clean.lower():
				product_hint = "–ö–ù"
				break
		# deposits
		if not product_hint:
			for k in ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑–∏—Ç","–¥–µ–ø–æ–∑" ]:
				if k in user_clean.lower():
					product_hint = "–í–∫–ª–∞–¥"
					break
	rag_texts, rag_meta = _rag_top_chunks(db, product_hint, user_clean, limit_docs=3, limit_chunks=5)
	ctx_text = "\n\n".join(rag_texts) if rag_texts else ""
	# Clarify currency/guards only in normal chats
	if not auto_summary:
		# Clarify currency if ambiguous
		detected_curr = _detect_currency(user_clean)
		if (not detected_curr) and rag_meta.get("currencies") and len(rag_meta["currencies"]) > 1:
			question = "–£—Ç–æ—á–Ω–∏—Ç–µ –≤–∞–ª—é—Ç—É: 1) RUB (‚ÇΩ), 2) USD ($), 3) EUR (‚Ç¨), 4) CNY (¬•)?"
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", question, off_topic=False)
			return question
		# Guard for KN/Deposit numeric citations
		if product_hint in ("–ö–ù","–í–∫–ª–∞–¥") and not rag_meta.get("rates"):
			msg = "–£—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–≤–∞–ª—é—Ç–∞/—Ç–∞—Ä–∏—Ñ/–∫–∞–Ω–∞–ª), –ø—Ä–∏—à–ª—é —Ü–∏—Ñ—Ä—ã."
			db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
			db.add_assistant_message(tg_id, "assistant", msg, off_topic=False)
			return msg
	try:
		db.log(tg_id, "rag_ctx", {"count": len(rag_texts) if rag_texts else 0, "previews": [t[:200] for t in (rag_texts or [])], "currencies": rag_meta.get("currencies", []), "via": rag_meta.get("via")})
	except Exception:
		pass

	messages: List[Dict[str, str]] = []
	messages.append({"role": "system", "content": _build_system_prompt(agent_name, stats_line + "; " + prev_line, group_line, notes_preview)})
	# Inject structured FACTS and SOURCES for downstream citation [F#]/[S#], except for auto-summary prompts
	auto_summary = "[auto_summary]" in user_clean.lower()
	if not auto_summary:
		# Compute day/week/month metrics to align FACTS with auto-summary
		try:
			# today
			today_total, _today_by = db._sum_attempts_query(tg_id, today, today)
			p_day = int(plan_info.get('plan_day', 0))
			c_day = (today_total * 100 / p_day) if p_day > 0 else 0
			# meetings and penetration for today
			m_day = db.meets_period_count(tg_id, today, today)
			linked_day = db.attempts_linked_period_count(tg_id, today, today)
			pen_day = (linked_day * 100 / m_day) if m_day > 0 else 0
			facts_lines: List[str] = []
			facts_lines.append(f"F1: –°–µ–≥–æ–¥–Ω—è —Ñ–∞–∫—Ç ‚Äî {today_total}")
			facts_lines.append(f"F2: –°–µ–≥–æ–¥–Ω—è –ø–ª–∞–Ω ‚Äî {p_day}")
			facts_lines.append(f"F3: –°–µ–≥–æ–¥–Ω—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_day))}")
			facts_lines.append(f"F4: –°–µ–≥–æ–¥–Ω—è –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ, % ‚Äî {int(round(pen_day))}")
			# minimal week/month anchors
			start_week = today - timedelta(days=today.weekday())
			week_total, _ = db._sum_attempts_query(tg_id, start_week, today)
			p_week = int(plan_info.get('plan_week', 0))
			c_week = (week_total * 100 / p_week) if p_week > 0 else 0
			facts_lines.append(f"F5: –ù–µ–¥–µ–ª—è —Ñ–∞–∫—Ç ‚Äî {week_total}")
			facts_lines.append(f"F6: –ù–µ–¥–µ–ª—è –ø–ª–∞–Ω ‚Äî {p_week}")
			facts_lines.append(f"F7: –ù–µ–¥–µ–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_week))}")
			start_month = today.replace(day=1)
			month_total, _ = db._sum_attempts_query(tg_id, start_month, today)
			p_month = int(plan_info.get('plan_month', 0))
			c_month = (month_total * 100 / p_month) if p_month > 0 else 0
			facts_lines.append(f"F8: –ú–µ—Å—è—Ü —Ñ–∞–∫—Ç ‚Äî {month_total}")
			facts_lines.append(f"F9: –ú–µ—Å—è—Ü –ø–ª–∞–Ω ‚Äî {p_month}")
			facts_lines.append(f"F10: –ú–µ—Å—è—Ü –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_month))}")
			facts_lines.append(f"F11: RR –º–µ—Å—è—Ü–∞ (–ø—Ä–æ–≥–Ω–æ–∑ —Ñ–∞–∫—Ç–∞) ‚Äî {int(plan_info.get('rr_month', 0))}")
			sources_lines: List[str] = []
			for i, s in enumerate((rag_meta.get("sources") or [])[:5], start=1):
				title = (s.get("title") or "–ò—Å—Ç–æ—á–Ω–∏–∫").strip()
				url = (s.get("url") or "").strip()
				sources_lines.append(f"S{i}: {title} ‚Äî {url}")
			fs_block = ("FACTS:\n" + "\n".join(facts_lines)) + ("\n\n" + ("SOURCES:\n" + "\n".join(sources_lines)) if sources_lines else "")
			messages.append({"role": "system", "content": fs_block})
		except Exception:
			pass
	if ctx_text:
		# Inject rate lines separately to anchor exact numbers; instruct to cite with [S#]
		rate_block = "\n".join(rag_meta.get("rates", []) or [])
		add = "–°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏; –≤ –æ—Ç–≤–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏ [S#] –Ω–∞ SOURCES, URL –Ω–µ –≤—Å—Ç–∞–≤–ª—è–π –Ω–∞–ø—Ä—è–º—É—é):\n" + ctx_text
		if rate_block:
			add += "\n\n–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ —Å—Ç–∞–≤–∫–∞–º–∏ (–∏—Å–ø–æ–ª—å–∑—É–π –¥–æ—Å–ª–æ–≤–Ω–æ –∏ –≤—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –≤–∞–ª—é—Ç—É):\n" + rate_block
		messages.append({"role": "system", "content": add})
	# Keep broader chat history for context; include last 20
	history = db.get_assistant_messages(tg_id, limit=20)
	for m in history:
		messages.append({"role": m["role"], "content": m["content_sanitized"]})
	messages.append({"role": "user", "content": user_clean})

	resp = client.chat.completions.create(
		model=settings.assistant_model,
		messages=messages,
		temperature=0.3,
		max_tokens=350,
	)
	answer = resp.choices[0].message.content or ""
	answer_clean = sanitize_text_assistant_output(answer)
	answer_clean = _normalize_bullets(answer_clean)
	answer_clean = _strip_md_emphasis(answer_clean)

	# Store
	db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
	db.add_assistant_message(tg_id, "assistant", answer_clean, off_topic=False)
	return answer_clean 


# ------------------------ Formatting helpers ------------------------

def _to_numbered(text: str) -> str:
	if not text:
		return ""
	# Normalize line breaks and split
	norm = text.replace("\r\n", "\n").replace("\r", "\n")
	raw_lines = [ln.strip() for ln in norm.split("\n") if ln.strip()]
	out: List[str] = []
	idx = 1
	for ln in raw_lines:
		# Keep section headers ending with ':' as-is
		if ln.endswith(":"):
			out.append(ln)
			continue
		# Strip common bullet markers
		clean = ln.lstrip("-‚Ä¢\t ")
		# Convert existing '1.' or '1)' to unified 'n)'
		m = re.match(r"^(\d{1,2})[)\.]+\s+(.*)", clean)
		if m:
			out.append(f"{idx}) {m.group(2)}")
		else:
			out.append(f"{idx}) {clean}")
		idx += 1
	return "\n".join(out) 