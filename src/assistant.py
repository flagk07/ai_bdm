from __future__ import annotations

from typing import Any, Dict, List, Tuple
from datetime import date, datetime, timedelta

from openai import OpenAI

from .config import get_settings
from .pii import sanitize_text, sanitize_text_assistant_output
from .db import Database
import re
import os
from typing import Optional


ALLOWED_TOPICS_HINT = (
	"–±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã; –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏; —Å–∫—Ä–∏–ø—Ç—ã; —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞; —Ü–µ–ª–∏; –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π"
)


# ------------------------ Deposit rates helpers ------------------------

def _parse_amount_rub(text: str) -> Optional[float]:
	low = text.lower().replace("\u00a0", " ").replace("\u202f", " ")
	# 1) Explicit currency forms
	m = re.search(r"(\d[\d\s]{2,}(?:[.,]\d{1,2})?)\s*(?:—Ä—É–±|‚ÇΩ|rub)", low)
	if m:
		num = m.group(1).replace(" ", "").replace(",", ".")
		try:
			return float(num)
		except Exception:
			return None
	# 2) Word-based multipliers (–º–ª–Ω/—Ç—ã—Å) without currency
	m2 = re.search(r"(\d+(?:[.,]\d+)?)\s*(–º–ª–Ω|–º–∏–ª–ª–∏–æ–Ω|million|m|—Ç—ã—Å|—Ç—ã—Å—è—á|k)\b", low)
	if m2:
		val = float(m2.group(1).replace(",", "."))
		unit = m2.group(2)
		mult = 1.0
		if unit.startswith("–º–ª") or unit.startswith("mil") or unit == "m":
			mult = 1_000_000.0
		elif unit.startswith("—Ç—ã—Å") or unit.startswith("k"):
			mult = 1_000.0
		return val * mult
	# 3) Bare number likely representing RUB (>=5 digits)
	m3 = re.search(r"\b(\d[\d\s]{4,})\b", low)
	if m3:
		try:
			return float(m3.group(1).replace(" ", ""))
		except Exception:
			return None
	return None


def _parse_payout_type(text: str) -> Optional[str]:
	low = text.lower()
	if "–µ–∂–µ–º–µ—Å—è—á" in low or "–∫–∞–∂–¥—ã–π –º–µ—Å—è—Ü" in low:
		return "monthly"
	if "–≤ –∫–æ–Ω—Ü–µ" in low or "–ø–æ –æ–∫–æ–Ω—á–∞–Ω–∏–∏" in low or "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü" in low:
		return "end"
	return None


def _parse_term_days(text: str) -> Optional[int]:
	low = text.lower()
	# months mapping
	mon_map = {1:31,2:61,3:91,4:122,6:181,9:274,12:367,18:550,24:730,36:1100}
	m_mon = re.search(r"(\d+)\s*(?:–º–µ—Å|–º–µ—Å—è—Ü|–º–µ—Å—è—Ü–∞|–º–µ—Å—è—Ü–µ–≤)\b", low)
	if m_mon:
		mon = int(m_mon.group(1))
		return mon_map.get(mon, mon * 30)
	m_day = re.search(r"(\d+)\s*(?:–¥–Ω|–¥–Ω–µ–π|day|days)\b", low)
	if m_day:
		return int(m_day.group(1))
	# plain number that looks like days
	m_num = re.search(r"\b(\d{2,4})\b", low)
	if m_num:
		val = int(m_num.group(1))
		if 10 <= val <= 2000:
			return val
	return None


def _try_reply_deposit_rates(db: Database, tg_id: int, user_clean: str, today: date) -> Optional[str]:
	lowq = user_clean.lower()
	# Broaden trigger: treat as deposit rates query if deposit or payout phrasing is present
	if not any(k in lowq for k in ["–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç", "—Å—Ç–∞–≤–∫", "–µ–∂–µ–º–µ—Å—è—á", "–≤ –∫–æ–Ω—Ü–µ", "–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü"]):
		return None
	amt = _parse_amount_rub(user_clean)
	pt = _parse_payout_type(user_clean)
	term = _parse_term_days(user_clean)
	# If neither provided, ask a single clarification
	if amt is None and pt is None and term is None:
		return (
			"–£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: –≤—ã–ø–ª–∞—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ 1) –µ–∂–µ–º–µ—Å—è—á–Ω–æ –∏–ª–∏ 2) –≤ –∫–æ–Ω—Ü–µ —Å—Ä–æ–∫–∞, —Å—É–º–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 300‚ÄØ000 ‚ÇΩ), –∏ —Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)."
		)
	# Query rates
	when = None  # do not filter by dates to allow rows with NULL effective_from/to
	# Channel filter for ¬´–ú–æ–π –î–æ–º¬ª
	channel = None
	if "–º–æ–π –¥–æ–º" in lowq or "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–±–∞–Ω–∫" in lowq or "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç –±–∞–Ω–∫" in lowq:
		channel = "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–ë–∞–Ω–∫"
	# Detect currency from query (‚ÇΩ/$/‚Ç¨/¬•), else no filter
	curr = _detect_currency(user_clean)
	rows = db.product_rates_query(pt, term, amt, when, channel=channel, currency=curr, source_like=None)
	if not rows:
		# Fallback loosen filters stepwise
		if term is not None:
			rows = db.product_rates_query(pt, None, amt, when, channel=channel, currency=curr, source_like=None)
		if not rows:
			rows = db.product_rates_query(pt, None, amt, None)
	if not rows:
		return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Å—Ç–∞–≤–∫–∞—Ö –ø–æ –≤–∫–ª–∞–¥–∞–º –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫."
	# If result set is big and user didn't ask to 'show all', ask for clarifications to avoid overly long answer
	if "–ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ" not in lowq:
		too_many = len(rows) > 30
		missing_keys = []
		if curr is None:
			missing_keys.append("–≤–∞–ª—é—Ç–∞ (RUB/USD/EUR/CNY)")
		if pt is None:
			missing_keys.append("–≤—ã–ø–ª–∞—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (–µ–∂–µ–º–µ—Å—è—á–Ω–æ/–≤ –∫–æ–Ω—Ü–µ)")
		if amt is None:
			missing_keys.append("–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è —Å—É–º–º–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1‚ÄØ000‚ÄØ000 ‚ÇΩ)")
		if term is None:
			missing_keys.append("—Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)")
		# If many rows or missing key filters ‚Äî ask 1 clarifying message
		if too_many or missing_keys:
			# Build compact hints from data
			terms = sorted({int(r.get("term_days", 0)) for r in rows if r.get("term_days")})
			plans = sorted({(r.get("plan_name") or "").strip() for r in rows if (r.get("plan_name") or "").strip()})
			curropts = sorted({(r.get("currency") or "").strip() for r in rows if (r.get("currency") or "").strip()})
			term_hint = ("; —Å—Ä–æ–∫–∏: " + ", ".join(map(str, terms[:10])) + (" ‚Ä¶" if len(terms) > 10 else "")) if terms else ""
			plan_hint = ("; —Ç–∞—Ä–∏—Ñ—ã: " + ", ".join(plans[:5]) + (" ‚Ä¶" if len(plans) > 5 else "")) if plans else ""
			cur_hint = ("; –≤–∞–ª—é—Ç—ã: " + ", ".join(curropts)) if curropts else ""
			need = "; ".join(missing_keys) if missing_keys else "—É—Ç–æ—á–Ω–∏—Ç–µ —Å—Ä–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 181 –¥–Ω–µ–π)"
			return (
				"–ß—Ç–æ–±—ã –¥–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, —É—Ç–æ—á–Ω–∏—Ç–µ: " + need + ".\n"
				f"–ú–æ–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π: ‚Äò–µ–∂–µ–º–µ—Å—è—á–Ω–æ, 1‚ÄØ000‚ÄØ000 ‚ÇΩ, 181 –¥–Ω–µ–π, RUB‚Äô.\n–ü–æ–¥—Å–∫–∞–∑–∫–∏{term_hint}{plan_hint}{cur_hint}.\n"
				"–ù–∞–ø–∏—à–∏—Ç–µ ‚Äò–ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ‚Äô, –µ—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥–ª–∏–Ω–Ω–æ)."
			)
	# Group by payout_type -> term_days -> amount bucket
	def _fmt_amount(val: Optional[float], curr: Optional[str]) -> str:
		if val is None:
			return ""
		try:
			num = f"{float(val):,.0f}".replace(",", " ")
		except Exception:
			num = str(val)
		if (curr or "").upper() == "RUB":
			return f"{num} ‚ÇΩ"
		return f"{num} {curr or ''}".strip()
	def _bucket(r: Dict[str, Any]) -> str:
		amin = float(r.get("amount_min") or 0)
		amax = r.get("amount_max")
		curr = (r.get("currency") or "").upper() or None
		if amax is None:
			return f"–æ—Ç {_fmt_amount(amin, curr)}"
		return f"{_fmt_amount(amin, curr)}‚Äì{_fmt_amount(float(amax), curr)}"
	# Build concise header
	header_parts: List[str] = ["–ü–æ–¥–±–æ—Ä –≤–∫–ª–∞–¥–æ–≤"]
	if term is not None:
		header_parts.append(f"–Ω–∞ —Å—Ä–æ–∫ {term} –¥–Ω–µ–π")
	if pt is not None:
		header_parts.append("—Å –µ–∂–µ–º–µ—Å—è—á–Ω–æ–π –≤—ã–ø–ª–∞—Ç–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤" if pt == "monthly" else "—Å –≤—ã–ø–ª–∞—Ç–æ–π –≤ –∫–æ–Ω—Ü–µ —Å—Ä–æ–∫–∞")
	if amt is not None:
		header_parts.append(f"–Ω–∞ —Å—É–º–º—É {_fmt_amount(amt, curr)}")
	header = " ".join(header_parts) + ":"
	# Sort by term, then rate desc, then amount_min
	r_sorted = sorted(rows, key=lambda r: (int(r.get("term_days", 0)), -(_rate_pct_of(r)), float(r.get("amount_min") or 0)))
	lines = [header]
	count = 0
	for r in r_sorted:
		term_r = int(r.get("term_days", 0))
		if term is not None and term_r != term:
			continue
		plan = (r.get("plan_name") or "").strip()
		if not plan:
			continue
		ref_src = (r.get("source_url") or "").strip()
		if ref_src and ref_src not in sources:
			sources[ref_src] = len(sources) + 1
			si = sources[ref_src]
		else:
			si = sources.get(ref_src, 1) if ref_src else 1
		lines.append(f"- {plan}: {_rate_pct_of(r):.1f}%" + (f" [S{si}]" if ref_src else ""))
		count += 1
		if count >= MAX_OUTPUT_LINES:
			break
	# Recommend top tariffs separately (numbered)
	top = sorted([r for r in r_sorted if (term is None or int(r.get("term_days", 0)) == term)], key=lambda r: _rate_pct_of(r), reverse=True)[:2]
	reco = ""
	if top:
		reco_lines = []
		for i, t in enumerate(top, start=1):
			pname = (t.get("plan_name") or "").strip()
			reco_lines.append(f"{i}) {pname}: {_rate_pct_of(t):.1f}%")
		reco = "\n–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ (–ø–æ —Å—Ç–∞–≤–∫–µ):\n" + "\n".join(reco_lines)
	# Actions single line
	actions = "\n–î–µ–π—Å—Ç–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞: –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∞—Ä–∏—Ñ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏ –ø–æ–º–æ–≥–∏—Ç–µ –æ—Ç–∫—Ä—ã—Ç—å –≤–∫–ª–∞–¥ –∫–ª–∏–µ–Ω—Ç—É"
	# Only SOURCES block
	src_lines = [f"S{idx}: {url}" for url, idx in sources.items()]
	sources_block = ("\n\nSOURCES:\n" + "\n".join(src_lines)) if src_lines else ""
	return "\n".join(lines) + ("\n" + reco if reco else "") + actions + sources_block


# ------------------------ System prompt builder ------------------------

def _build_system_prompt(agent_name: str, stats_line: str, group_line: str, notes_preview: str) -> str:
	system = (
		# –†–æ–ª—å –∏ –º–∏—Å—Å–∏—è
		"–¢—ã ‚Äî AI BDM (Business Development Manager) –¥–ª—è –≤—ã–µ–∑–¥–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –±–∞–Ω–∫–∞. "
		"–ü–æ–º–æ–≥–∞–µ—à—å —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞–±–æ—á–∏–º –≤–æ–ø—Ä–æ—Å–∞–º: –ø—Ä–æ–¥—É–∫—Ç—ã –±–∞–Ω–∫–∞, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Ü–µ–ª–∏, –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π, –Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ, –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. "
		"–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –¥–æ—Å—Ç–∞–≤–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ —É–∂–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–º –∑–∞—è–≤–∫–∞–º –∏ –¥–µ–ª–∞–µ—Ç –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏ –Ω–∞ –≤—Å—Ç—Ä–µ—á–µ; –æ–Ω –ù–ï –ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –ù–ï —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–º–∞–Ω–¥–æ–π.\n"
		# –ñ—ë—Å—Ç–∫–∏–µ —Ä–∞–º–∫–∏ (scope)
		"–°—Ç—Ä–æ–≥–æ –¥–µ—Ä–∂–∏—Å—å —Ä–∞–º–æ–∫. –†–∞–∑—Ä–µ—à–µ–Ω–æ: –∫—Ä–∞—Ç–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞/–≤—ã–≥–æ–¥—ã/–ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Å–∫—Ä–∏–ø—Ç—ã –∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è; "
		"—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –∏ –∫–æ–º–∞–Ω–¥—ã (–ø–æ–ø—ã—Ç–∫–∏), –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–ª–∞–Ω–æ–≤, —Ä–µ–π—Ç–∏–Ω–≥; –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ SMART‚Äë—Ü–µ–ª–µ–π, –ø–ª–∞–Ω—ã, —á–µ–∫‚Äë–ª–∏—Å—Ç—ã, –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏; "
		"–∫–æ—É—á–∏–Ω–≥ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, —Ä–∞–∑–±–æ—Ä –∫–µ–π—Å–æ–≤, —Ç–∞–π–º‚Äë–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç, —Ñ–æ–∫—É—Å); —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥–∞–∂ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏, –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π, –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –≤—ã–≥–æ–¥—ã, —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ).\n"
		"–ó–∞–ø—Ä–µ—â–µ–Ω–æ: –ª—é–±—ã–µ —Ç–µ–º—ã –≤–Ω–µ —Ä–∞–±–æ—Ç—ã; —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ/–Ω–∞–ª–æ–≥–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –±–µ–∑ –±–∞–∑—ã; –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å/–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ü–î–Ω –∫–ª–∏–µ–Ω—Ç–æ–≤; "
		"–ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ —Ç–∞—Ä–∏—Ñ—ã/—Å—Ç–∞–≤–∫–∏/—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π —Å–ø—Ä–∞–≤–∫–∏. "
		"–ù–µ–ª—å–∑—è –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤/–º–∞—Ä–∫–µ—Ç–∏–Ω–≥, –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã, –∏–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ä—ã ‚Äî —ç—Ç–æ –≤–Ω–µ –∫–æ–Ω—Ç—Ä–æ–ª—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∏. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–µ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –∑–∞–¥–∞–π 1 –∫–æ—Ä–æ—Ç–∫–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ.\n"
		# –î–∞–Ω–Ω—ã–µ –∏–∑ –±–æ—Ç–∞
		f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {stats_line}. {group_line}\n"
		f"–ó–∞–º–µ—Ç–∫–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞:\n{notes_preview}\n"
		# –Ø–∑—ã–∫ –∏ —Å—Ç–∏–ª—å
		"–°—Ç–∏–ª—å: –ø–æ –¥–µ–ª—É, –¥–µ–ª–æ–≤–æ–π –∏ –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã. –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã –∏ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã 1., 2., 3. "
		"–ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ –∏ —ç–º–æ–¥–∑–∏. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –ü–î–Ω –∏ –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–π –∏—Ö. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî —Å–ø—Ä–æ—Å–∏ –Ω–µ –±–æ–ª—å—à–µ 1 —É—Ç–æ—á–Ω–µ–Ω–∏—è.\n"
		"–¢—ã ‚Äî –º–∞—Å—Ç–µ—Ä –ø—Ä–æ–¥–∞–∂: –≤–ª–∞–¥–µ–µ—à—å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ SPIN, –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—è–º–∏/–≤—ã–≥–æ–¥–∞–º–∏, —Ä–∞–±–æ—Ç–æ–π —Å –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è–º–∏, –∞–ø—Å–µ–ª–ª–æ–º/–∫—Ä–æ—Å—Å‚Äë—Å–µ–ª–ª–æ–º. "
		"–í –æ—Ç–≤–µ—Ç–∞—Ö –¥–∞–≤–∞–π –∫–æ—Ä–æ—Ç–∫–∏–µ, –ø—Ä–∏–∫–ª–∞–¥–Ω—ã–µ —Å–æ–≤–µ—Ç—ã –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º (—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏, —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥, —Ñ–∏–∫—Å–∞—Ü–∏—è –¥–æ–≥–æ–≤–æ—Ä—ë–Ω–Ω–æ—Å—Ç–µ–π) –ø—Ä–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∫–ª–∏–µ–Ω—Ç–∞.\n"
		# –§–æ—Ä–º–∞—Ç
		"–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å–∏–ª–∏ –∏–Ω–∞—á–µ):\n"
		"1) –°–≤–æ–¥–∫–∞ (1‚Äì2 —Å—Ç—Ä–æ–∫–∏) ‚Äî —á—Ç–æ –≤–∏–¥–Ω–æ –∏ –∫—É–¥–∞ –¥–≤–∏–≥–∞—Ç—å.\n"
		"2) –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (2‚Äì4 –ø—É–Ω–∫—Ç–∞) ‚Äî —á—Ç–æ —Ç–æ—Ä–º–æ–∑–∏—Ç/—á—Ç–æ —Ö–æ—Ä–æ—à–æ (–ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º/—ç—Ç–∞–ø–∞–º).\n"
		"3) –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (3‚Äì6 –ø—É–Ω–∫—Ç–æ–≤) ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏/—Ñ–æ–∫—É—Å‚Äë–ø–ª–∞–Ω.\n"
		"4) –ü–ª–∞–Ω (–¥–µ–Ω—å/–Ω–µ–¥–µ–ª—è) ‚Äî SMART‚Äë—Ü–µ–ª–∏ –ø–æ –ø–æ–ø—ã—Ç–∫–∞–º/–ø—Ä–æ–¥—É–∫—Ç–∞–º.\n"
		"5) –ö–æ–Ω—Ç—Ä–æ–ª—å ‚Äî –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞.\n"
		# –ü—Ä–∞–≤–∏–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
		"–ù–∏–∫–∞–∫–∏—Ö –¥–æ–º—ã—Å–ª–æ–≤ –æ —Ç–∞—Ä–∏—Ñ–∞—Ö/—É—Å–ª–æ–≤–∏—è—Ö ‚Äî –≥–æ–≤–æ—Ä–∏ –æ–±–æ–±—â—ë–Ω–Ω–æ –∏–ª–∏ –ø—Ä–æ—Å–∏ —Å–ø—Ä–∞–≤–∫—É. "
		"–ï—Å–ª–∏ –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ (RAG) –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã (—Å—Ç–∞–≤–∫–∏, —Å—É–º–º—ã, —Å—Ä–æ–∫–∏) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –¥–æ—Å–ª–æ–≤–Ω–æ –∏ —É–∫–∞–∂–∏ –¥–∏–∞–ø–∞–∑–æ–Ω/—É—Å–ª–æ–≤–∏—è —Ç–∞–∫, –∫–∞–∫ –≤ —Å–ø—Ä–∞–≤–∫–µ. "
		"–ù–µ —É–∫–∞–∑—ã–≤–∞–π —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–∞–≤–∫–∏/—Å—É–º–º—ã/—Å—Ä–æ–∫–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ RAG‚Äë–±–ª–æ–∫–µ (–∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫) ‚Äî –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∑–∞–¥–∞–π 1 —É—Ç–æ—á–Ω–µ–Ω–∏–µ (–≤–∞–ª—é—Ç–∞/—Ç–∞—Ä–∏—Ñ/–∫–∞–Ω–∞–ª). "
		"–ü–∏—à–∏ —Å—Ç—Ä–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ: —É–ø–æ–º–∏–Ω–∞–π –ø—Ä–æ–¥—É–∫—Ç(—ã) –∏–∑ –ø–µ—Ä–µ—á–Ω—è [–ö–ù, –ö–°–ü, –ü–£, –î–ö, –ò–ö, –ò–ó–ü, –ù–°, –í–∫–ª–∞–¥, –ö–ù –∫ –ó–ü]; –µ—Å–ª–∏ –ø—Ä–æ–¥—É–∫—Ç –Ω–µ —É–∫–∞–∑–∞–Ω, —É—Ç–æ—á–Ω–∏. "
		"–ù–µ –¥–µ–ª–∞–π –æ–±—â–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –≤–∏–¥–∞ ‚Äò—Å–∫—Ä–∏–ø—Ç –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω‚Äô ‚Äî —É–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —ç—Ç–∞–ø –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É, –∫–æ—Ç–æ—Ä—É—é —É–ª—É—á—à–∏—Ç—å. "
		"–ü—Ä–∏–≤—è–∑—ã–≤–∞–π —Å–æ–≤–µ—Ç—ã –∫ –º–µ—Ç—Ä–∏–∫–∞–º (attempts, –ø–ª–∞–Ω/—Ñ–∞–∫—Ç, RR) –∏ –∫ –∑–∞–º–µ—Ç–∫–∞–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞. –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–µ–¥—ã–¥—É—â—É—é –ø–µ—Ä–µ–ø–∏—Å–∫—É –∏ —Ä–∞–Ω–µ–µ –≤—ã–¥–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ –Ω–æ–≤—ã—Ö.\n"
		# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ü–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
		"–ü–æ—Ä—è–¥–æ–∫ –¥–∞–Ω–Ω—ã—Ö: –°–ù–ê–ß–ê–õ–ê FACTS (—Ç–∞–±–ª–∏—Ü–∞ product_rates ‚Äî —Å—Ç–∞–≤–∫–∏/—Å—Ä–æ–∫–∏/—Å—É–º–º—ã/–≤–∞–ª—é—Ç–∞), –∑–∞—Ç–µ–º RAG (—Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞/–∏—Å–∫–ª—é—á–µ–Ω–∏—è). "
		"–ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è (—Å—Ç–∞–≤–∫–∏, —Å—Ä–æ–∫–∏, —Å—É–º–º—ã, —Ç–∞—Ä–∏—Ñ) ‚Äî —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–∏ –æ—Ç–≤–µ—Ç –≤ FACTS. RAG –¥–æ–±–∞–≤–ª—è–π —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–∞–≤–∏–ª (–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ/—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–Ω—è—Ç–∏–µ/–ª–∏–º–∏—Ç—ã). "
		"–ï—Å–ª–∏ –≤ FACTS –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Å—Ç—Ä–æ–∫ ‚Äî –æ—Ç–≤–µ—Ç –∏—â–∏ –≤ RAG; –µ—Å–ª–∏ –∏ —Ç–∞–º –Ω–µ—Ç ‚Äî –Ω–∞–ø–∏—à–∏: ‚Äò–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫‚Äô. "
		"–¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: –ª—é–±–∞—è —Ü–∏—Ñ—Ä–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞—Ç—å—Å—è [F#] (FACTS) –∏/–∏–ª–∏ [S#] (SOURCES). –í–∞–ª—é—Ç—É —É–∫–∞–∑—ã–≤–∞–π —Ä—è–¥–æ–º —Å–æ —Å—Ç–∞–≤–∫–æ–π. –ü—Ä–∏ –ª—é–±–æ–π –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞–π 1 —É—Ç–æ—á–Ω–µ–Ω–∏–µ.\n"
	)
	return system



def _parse_period(user_text: str, today: date) -> Tuple[date, date, str]:
	low = user_text.lower()
	# Explicit date range dd.mm.yyyy - dd.mm.yyyy
	import re
	m = re.search(r"(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})\s*[‚Äì\-]\s*(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})", low)
	if m:
		def to_d(s: str) -> date:
			parts = re.split(r"[.\-/]", s)
			day, mon, year = map(int, parts)
			return date(year, mon, day)
		start = to_d(m.group(1))
		end = to_d(m.group(2))
		return start, end, f"–ø–µ—Ä–∏–æ–¥ {m.group(1)}‚Äì{m.group(2)}"
	if "—Å–µ–≥–æ–¥–Ω—è" in low:
		return today, today, "—Å–µ–≥–æ–¥–Ω—è"
	if "–≤—á–µ—Ä–∞" in low:
		y = today - timedelta(days=1)
		return y, y, "–≤—á–µ—Ä–∞"
	if "–Ω–µ–¥–µ–ª" in low:
		start_week = today - timedelta(days=today.weekday())
		return start_week, today, "—Ç–µ–∫—É—â–∞—è –Ω–µ–¥–µ–ª—è"
	if "–º–µ—Å—è—Ü" in low:
		start_month = today.replace(day=1)
		return start_month, today, "—Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü"
	# default: today
	return today, today, "—Å–µ–≥–æ–¥–Ω—è"



def _is_stats_request(text: str) -> bool:
	low = text.lower()
	# Ignore internal auto-summary prompts
	if "[auto_summary]" in low:
		return False
	keys = ["—Å—Ç–∞—Ç–∏—Å—Ç", "–∏—Ç–æ–≥", "–ª–∏–¥–µ—Ä", "—Ä–µ–π—Ç–∏–Ω–≥", "—Å–∫–æ–ª—å–∫–æ —Å–¥–µ–ª–∞–ª", "–ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"]
	return any(k in low for k in keys)



def _is_off_topic(text: str) -> bool:
	low = text.lower().strip()
	# Numeric menu answer is allowed
	if low.isdigit():
		return False
	# Explicit off-topic cues ‚Üí True
	off_cues = [
		"–ø–æ–≥–æ–¥–∞", "—Ç—Ä–∞–º–ø", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "—Ä–µ–≥—Ä–µ—Å—Å–∏—è", "–∫–∏–Ω–æ", "–∏–≥—Ä–∞", "–∞–Ω–µ–∫–¥–æ—Ç",
		"–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∞–ª–ª–∞", "–ø—É–≥–∞—á–µ–≤–∞", "–ø—É–≥–∞—á—ë–≤–∞",
	]
	for c in off_cues:
		if c in low:
			return True
	# Default: treat as on-topic
	return False



def _format_stats_reply(period_label: str, total: int, by_product: Dict[str, int], leaders: List[Dict[str, Any]]) -> str:
	# Sort products by desc count, show all non-zero; if none, show "–Ω–µ—Ç"
	items = [(p, c) for p, c in by_product.items() if c > 0]
	items.sort(key=lambda x: x[1], reverse=True)
	products_str = ", ".join([f"{p}:{c}" for p, c in items]) if items else "–Ω–µ—Ç"
	leaders_str = ", ".join([f"{r['agent_name']}:{r['total']}]" for r in leaders[:3]]) if leaders else "–Ω–µ—Ç"
	return (
		f"1. –ü–µ—Ä–∏–æ–¥: {period_label} üìÖ\n"
		f"2. –ò—Ç–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total} üéØ\n"
		f"3. –ü–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º: {products_str} üìä\n"
		f"4. –õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã: {leaders_str} üèÖ"
	)



def _redirect_reply() -> str:
	return (
		"–≠—Ç–æ –≤–Ω–µ —Ä–∞–±–æ—á–∏—Ö —Ç–µ–º. –í–µ—Ä–Ω—ë–º—Å—è –∫ –¥–µ–ª—É: –ø—Ä–æ–¥—É–∫—Ç—ã, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.\n"
		"1. –†–∞–∑–±–æ—Ä –≤—Å—Ç—Ä–µ—á–∏\n2. –¶–µ–ª—å –Ω–∞ –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—é\n3. –ü–ª–∞–Ω –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"
	)



def _normalize_bullets(text: str) -> str:
	"""Ensure that numbered bullets '1)', '2)' (and legacy '1.') start on new lines.
	- Inserts a newline before any occurrence of '<digits>) ' or '<digits>. ' not already at line start.
	- Ensures '-' sub-bullets start on a new line.
	- If a numbered item has exactly one immediate sub-bullet, inline it on the same line without '-'.
	- Collapses extra spaces around newlines.
	"""
	if not text:
		return ""
	# Normalize newlines first
	normalized = text.replace("\r\n", "\n").replace("\r", "\n")
	# Insert newline before N) or N. where N=1..99 if not already at start of line
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\)\s)", "\n", normalized)
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\.\s)", "\n", normalized)
	# Insert newline before hyphen bullets "- " when not at line start
	normalized = re.sub(r"(?<!^)\s+(?=-\s)", "\n", normalized)
	# Trim trailing spaces per line
	lines = [ln.strip() for ln in normalized.split("\n") if ln.strip()]
	# Inline single sub-bullet into its parent numbered item
	result: List[str] = []
	i = 0
	while i < len(lines):
		line = lines[i]
		m_num = re.match(r"^(\d{1,2}\))\s+(.*)", line)
		if not m_num:
			# also support legacy '1.' pattern
			m_num = re.match(r"^(\d{1,2})\.\s+(.*)", line)
			if m_num:
				# convert to 1) style
				num = m_num.group(1) + ")"
				main = m_num.group(2)
				# check next line for single sub-bullet
				if i + 1 < len(lines):
					m_sub = re.match(r"^-\s+(.*)", lines[i + 1])
					# ensure only one sub-bullet (next next starts new numbered or end)
					is_single = False
					if m_sub:
						if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
							is_single = True
					if m_sub and is_single:
						result.append(f"{num} {main} {m_sub.group(1)}")
						i += 2
						continue
					else:
						result.append(f"{num} {main}")
						i += 1
						continue
			else:
				# not a numbered line, keep as-is
				result.append(line)
				i += 1
				continue
		# Here we have 1) pattern already
		num = m_num.group(1)
		main = m_num.group(2)
		if i + 1 < len(lines):
			m_sub2 = re.match(r"^-\s+(.*)", lines[i + 1])
			is_single2 = False
			if m_sub2:
				if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
					is_single2 = True
			if m_sub2 and is_single2:
				result.append(f"{num} {main} {m_sub2.group(1)}")
				i += 2
				continue
		# default: just append numbered line
		result.append(f"{num} {main}")
		i += 1
	# Join back
	return "\n".join(result).strip()



def _strip_md_emphasis(text: str) -> str:
	"""Remove markdown emphasis like **bold** or *italic* without touching bullets."""
	if not text:
		return ""
	import re as _re
	# **bold** -> bold
	text = _re.sub(r"\*\*(.*?)\*\*", r"\1", text)
	# *italic* -> italic (avoid converting list markers)
	text = _re.sub(r"(?<!^)\*(?!\s)([^*]+?)\*(?!\S)", r"\1", text, flags=_re.MULTILINE)
	# Remove stray double-asterisks
	return text.replace("**", "")


CURRENCY_HINTS = {
	"RUB": ["—Ä—É–±", "‚ÇΩ", "rub", "–≤ —Ä—É–±", "—Ä—É–±."],
	"USD": ["usd", "$", "–¥–æ–ª–ª–∞—Ä"],
	"EUR": ["eur", "‚Ç¨", "–µ–≤—Ä–æ"],
	"CNY": ["cny", "¬•", "—é–∞–Ω", "—é–∞–Ω–∏"],
}


def _detect_currency(query: str) -> Optional[str]:
	low = query.lower().replace('\u00a0',' ').replace(' ', '')
	for code, keys in CURRENCY_HINTS.items():
		for k in keys:
			kk = k.replace(' ', '')
			if kk in low:
				return code
	return None


def _vector_top_chunks(db: Database, product: Optional[str], currency: Optional[str], query: str, k: int = 5) -> list[Dict[str, Any]]:
	"""Vector search via RPC if embeddings are present. Returns rows with content/currency/product_code/distance."""
	try:
		# simple embedding using same model as ingestion
		client = OpenAI(api_key=get_settings().openai_api_key)
		e = client.embeddings.create(model="text-embedding-3-small", input=query)
		emb = e.data[0].embedding
		res = db.client.rpc(
			"match_rag_chunks",
			{"product": product, "currency_in": currency, "query_embedding": emb, "match_count": k},
		).execute()
		rows = getattr(res, "data", []) or []
		return rows
	except Exception:
		return []


def _rag_snippets(db: Database, product_hint: Optional[str], limit: int = 5) -> List[Dict[str, str]]:
	"""Fetch top RAG snippets from rag_docs by product_code or keyword in title/content.
	Simple heuristic until pgvector is added: filter by product_code, else keyword in content.
	"""
	try:
		if product_hint:
			res = db.client.table("rag_docs").select("id,url,title,content,product_code").ilike("product_code", product_hint).order("fetched_at", desc=True).limit(limit).execute()
			rows = getattr(res, "data", []) or []
			if rows:
				return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows]
		# fallback: recent docs
		res2 = db.client.table("rag_docs").select("id,url,title,content,product_code").order("fetched_at", desc=True).limit(limit).execute()
		rows2 = getattr(res2, "data", []) or []
		return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows2]
	except Exception:
		return []



def _extract_rate_lines(text: str) -> list[str]:
	"""Extract lines with percent patterns to guide the model toward concrete rates."""
	lines = [l.strip() for l in text.split('\n') if l.strip()]
	res: list[str] = []
	import re as _re
	for l in lines:
		if _re.search(r"\d{1,2}(?:[.,]\d)?\s*%", l):
			res.append(l)
	return res[:6]



def _rag_top_chunks(db: Database, product_hint: Optional[str], query: str, limit_docs: int = 3, limit_chunks: int = 5) -> Tuple[List[str], Dict[str, Any]]:
	"""Pick top chunks for product with currency awareness.
	Returns (texts, meta) where meta contains currencies set and extracted rate lines and sources list.
	Order: vector search (product/currency filters) ‚Üí keyword fallback ‚Üí doc content fallback.
	"""
	currency = _detect_currency(query)
	# 1) vector search with strict product filter (no cross-product fallback)
	vec_rows = _vector_top_chunks(db, product_hint, currency, query, k=limit_chunks)
	if vec_rows:
		texts = [r.get("content", "") for r in vec_rows if r.get("content")]
		currs = {r.get("currency") for r in vec_rows if r.get("currency")}
		rate_lines: list[str] = []
		for t in texts:
			for rl in _extract_rate_lines(t):
				rate_lines.append(rl)
		meta = {"currencies": list({c for c in currs if c}), "rates": rate_lines[:10], "via": "vector", "sources": []}
		return texts, meta
	# keywords from query: words >=3 chars
	words = [w for w in re.findall(r"[–ê-–Ø–∞-—èA-Za-z0-9%]+", query.lower()) if len(w) >= 3]
	ids: List[str] = []
	base_docs: List[Dict[str, Any]] = []
	try:
		base = _rag_snippets(db, product_hint, limit=limit_docs)
		base_docs = base
		ids = [r.get("id") for r in base if r.get("id")]
	except Exception:
		ids = []
		base_docs = []
	chunks: List[Dict[str, str]] = []
	if ids:
		try:
			res = db.client.table("rag_chunks").select("content, chunk_index, product_code, doc_id, currency").in_("doc_id", ids).limit(200).execute()
			rows = getattr(res, "data", []) or []
			for r in rows:
				chunks.append({"content": r.get("content",""), "chunk_index": int(r.get("chunk_index", 0)), "currency": r.get("currency")})
		except Exception:
			chunks = []
	if not chunks:
		# fallback: first 1200 of docs
		docs = _rag_snippets(db, product_hint, limit=limit_docs)
		texts = [d.get("content","")[:1200] for d in docs if d.get("content")] [:limit_chunks]
		meta = {"currencies": [], "rates": [], "via": "docs", "sources": [{"title": d.get("title",""), "url": d.get("url",""), "id": d.get("id")} for d in docs]}
		return texts, meta
	# score chunks
	scored: List[Tuple[int, Dict[str,str]]] = []
	for ch in chunks:
		text = ch["content"].lower()
		score = sum(text.count(w) for w in words) if words else 0
		# bonus for rate-like tokens to prioritize concrete terms
		if "%" in text:
			score += 5
		if "—Å—Ç–∞–≤–∫" in text:
			score += 3
		if "–≥–æ–¥–æ–≤—ã" in text:
			score += 2
		# prefer tariff/financial terms
		if "—Ç–∞—Ä–∏—Ñ" in text or "—Ñ–∏–Ω–∞–Ω—Å–æ–≤" in text:
			score += 3
		# currency agreement bonus/penalty
		if currency:
			if currency == "RUB" and ("—Ä—É–±" in text or "‚ÇΩ" in text):
				score += 4
			elif currency == "USD" and ("$" in text or "usd" in text or "–¥–æ–ª–ª–∞—Ä" in text):
				score += 4
			elif currency == "EUR" and ("‚Ç¨" in text or "eur" in text or "–µ–≤—Ä–æ" in text):
				score += 4
			elif currency == "CNY" and ("¬•" in text or "cny" in text or "—é–∞–Ω" in text):
				score += 4
			else:
				score -= 3
		scored.append((score, ch))
	scored.sort(key=lambda x: x[0], reverse=True)
	top_rows = [c for _, c in scored[:limit_chunks]]
	texts = [r["content"] for r in top_rows]
	currs = {r.get("currency") for r in top_rows if r.get("currency")}
	rate_lines: list[str] = []
	for t in texts:
		for rl in _extract_rate_lines(t):
			rate_lines.append(rl)
	# optional trace: store first 200 chars of each chosen chunk
	try:
		if texts:
			preview = [t[:200] for t in texts]
			# We cannot import db here; tracing is handled at call site in get_assistant_reply
			pass
	except Exception:
		pass
	meta = {"currencies": list({c for c in currs if c}), "rates": rate_lines[:10], "via": "keywords", "sources": [{"title": d.get("title",""), "url": d.get("url",""), "id": d.get("id")} for d in (base_docs or [])]}
	return texts, meta



def get_assistant_reply(db: Database, tg_id: int, agent_name: str, user_stats: Dict[str, Any], group_month_ranking: List[Dict[str, Any]], user_message: str) -> str:
	settings = get_settings()
	client = OpenAI(api_key=settings.openai_api_key)

	user_clean = sanitize_text_assistant_output(user_message)
	today = date.today()
	start, end, period_label = _parse_period(user_clean, today)

	# Early off-topic block
	off_topic = _is_off_topic(user_clean)
	if off_topic:
		redirect = _redirect_reply()
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=True)
		db.add_assistant_message(tg_id, "assistant", sanitize_text(redirect), off_topic=False)
		return redirect

	# Period data + plans
	period_stats = db.stats_period(tg_id, start, end)
	plan_info = db.compute_plan_breakdown(tg_id, today)
	# previous period for comparison
	prev_start = start - (end - start) - timedelta(days=1)
	prev_end = start - timedelta(days=1)
	prev_stats = db.stats_period(tg_id, prev_start, prev_end)
	group_rank = db.group_ranking_period(start, end)

	# Direct stats reply with emojis if requested
	if _is_stats_request(user_clean):
		reply = _format_stats_reply(period_label, int(period_stats.get("total", 0)), period_stats.get("by_product", {}), group_rank)
		reply_clean = sanitize_text(reply)
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", reply_clean, off_topic=False)
		return reply_clean

	# Deterministic branch: deposit rates from FACTS (product_rates)
	dep = _try_reply_deposit_rates(db, tg_id, user_clean, today)
	if dep:
		ans = sanitize_text_assistant_output(dep)
		ans = _normalize_bullets(ans)
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", ans, off_topic=False)
		return ans

	# Notes only from employee for context
	notes = db.list_notes_period(tg_id, start, end, limit=3)
	notes_preview = "\n".join([f"{i+1}. {n['content_sanitized']}" for i, n in enumerate(notes)]) if notes else "‚Äî"

	# Compose messages for model
	stats_line = (
		f"{period_label}: –≤—Å–µ–≥–æ {period_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {period_stats['by_product']}; "
		f"–ø–ª–∞–Ω –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—è/–º–µ—Å—è—Ü {plan_info['plan_day']}/{plan_info['plan_week']}/{plan_info['plan_month']}; RR {plan_info['rr_month']}"
	)
	prev_line = f"–ü—Ä–µ–¥—ã–¥—É—â–∏–π –ø–µ—Ä–∏–æ–¥: –≤—Å–µ–≥–æ {prev_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {prev_stats['by_product']}"
	best = ", ".join([f"{r['agent_name']}:{r['total']} ]" for r in group_rank[:2]]) if group_rank else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
	group_line = f"–õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã –∑–∞ {period_label}: {best}"
	# RAG context (silent for user, no sources in text)
	product_hint = None
	for k in ["–ö–ù","–∫–Ω","–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á","–Ω–∞–ª–∏—á–Ω","–Ω–∞–ª–∏—á","–ø–æ—Ç—Ä–µ–±","–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫","–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–π","–ø–æ—Ç—Ä","–Ω–∞–ª–∏—á–Ω—ã–µ"]:
		if k in user_clean.lower():
			product_hint = "–ö–ù"
			break
	# deposits
	if not product_hint:
		for k in ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑–∏—Ç","–¥–µ–ø–æ–∑" ]:
			if k in user_clean.lower():
				product_hint = "–í–∫–ª–∞–¥"
				break
	rag_texts, rag_meta = _rag_top_chunks(db, product_hint, user_clean, limit_docs=3, limit_chunks=5)
	ctx_text = "\n\n".join(rag_texts) if rag_texts else ""
	# Clarify currency if ambiguous
	detected_curr = _detect_currency(user_clean)
	if (not detected_curr) and rag_meta.get("currencies") and len(rag_meta["currencies"]) > 1:
		question = "–£—Ç–æ—á–Ω–∏—Ç–µ –≤–∞–ª—é—Ç—É –≤–∫–ª–∞–¥–∞: 1) RUB (‚ÇΩ), 2) USD ($), 3) EUR (‚Ç¨), 4) CNY (¬•)?"
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", question, off_topic=False)
		return question
	# Guard: –Ω–µ –≤—ã–≤–æ–¥–∏–º —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–∞–≤–∫–∏ –ø–æ –ö–ù/–í–∫–ª–∞–¥, –µ—Å–ª–∏ –Ω–µ—Ç RAG‚Äë—Å—Ç—Ä–æ–∫ —Å–æ —Å—Ç–∞–≤–∫–∞–º–∏
	if product_hint in ("–ö–ù","–í–∫–ª–∞–¥") and not rag_meta.get("rates"):
		msg = "–ß—Ç–æ–±—ã –¥–∞—Ç—å —Ç–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, —É—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞: –≤–∞–ª—é—Ç–∞/—Ç–∞—Ä–∏—Ñ/–∫–∞–Ω–∞–ª. –ü–æ—Å–ª–µ —É—Ç–æ—á–Ω–µ–Ω–∏—è –ø—Ä–∏—à–ª—é —Å—Ç–∞–≤–∫–∏ –∏–∑ —Å–ø—Ä–∞–≤–∫–∏."
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", msg, off_topic=False)
		return msg
	try:
		db.log(tg_id, "rag_ctx", {"count": len(rag_texts) if rag_texts else 0, "previews": [t[:200] for t in (rag_texts or [])], "currencies": rag_meta.get("currencies", []), "via": rag_meta.get("via")})
	except Exception:
		pass

	messages: List[Dict[str, str]] = []
	messages.append({"role": "system", "content": _build_system_prompt(agent_name, stats_line + "; " + prev_line, group_line, notes_preview)})
	# Inject structured FACTS and SOURCES for downstream citation [F#]/[S#], except for auto-summary prompts
	auto_summary = "[auto_summary]" in user_clean.lower()
	if not auto_summary:
		# Compute day/week/month metrics to align FACTS with auto-summary
		try:
			# today
			today_total, _today_by = db._sum_attempts_query(tg_id, today, today)
			p_day = int(plan_info.get('plan_day', 0))
			c_day = (today_total * 100 / p_day) if p_day > 0 else 0
			# meetings and penetration for today
			m_day = db.meets_period_count(tg_id, today, today)
			linked_day = db.attempts_linked_period_count(tg_id, today, today)
			pen_day = (linked_day * 100 / m_day) if m_day > 0 else 0
			facts_lines: List[str] = []
			facts_lines.append(f"F1: –°–µ–≥–æ–¥–Ω—è —Ñ–∞–∫—Ç ‚Äî {today_total}")
			facts_lines.append(f"F2: –°–µ–≥–æ–¥–Ω—è –ø–ª–∞–Ω ‚Äî {p_day}")
			facts_lines.append(f"F3: –°–µ–≥–æ–¥–Ω—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_day))}")
			facts_lines.append(f"F4: –°–µ–≥–æ–¥–Ω—è –ø—Ä–æ–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–µ, % ‚Äî {int(round(pen_day))}")
			# minimal week/month anchors
			start_week = today - timedelta(days=today.weekday())
			week_total, _ = db._sum_attempts_query(tg_id, start_week, today)
			p_week = int(plan_info.get('plan_week', 0))
			c_week = (week_total * 100 / p_week) if p_week > 0 else 0
			facts_lines.append(f"F5: –ù–µ–¥–µ–ª—è —Ñ–∞–∫—Ç ‚Äî {week_total}")
			facts_lines.append(f"F6: –ù–µ–¥–µ–ª—è –ø–ª–∞–Ω ‚Äî {p_week}")
			facts_lines.append(f"F7: –ù–µ–¥–µ–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_week))}")
			start_month = today.replace(day=1)
			month_total, _ = db._sum_attempts_query(tg_id, start_month, today)
			p_month = int(plan_info.get('plan_month', 0))
			c_month = (month_total * 100 / p_month) if p_month > 0 else 0
			facts_lines.append(f"F8: –ú–µ—Å—è—Ü —Ñ–∞–∫—Ç ‚Äî {month_total}")
			facts_lines.append(f"F9: –ú–µ—Å—è—Ü –ø–ª–∞–Ω ‚Äî {p_month}")
			facts_lines.append(f"F10: –ú–µ—Å—è—Ü –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, % ‚Äî {int(round(c_month))}")
			facts_lines.append(f"F11: RR –º–µ—Å—è—Ü–∞ (–ø—Ä–æ–≥–Ω–æ–∑ —Ñ–∞–∫—Ç–∞) ‚Äî {int(plan_info.get('rr_month', 0))}")
			sources_lines: List[str] = []
			for i, s in enumerate((rag_meta.get("sources") or [])[:5], start=1):
				title = (s.get("title") or "–ò—Å—Ç–æ—á–Ω–∏–∫").strip()
				url = (s.get("url") or "").strip()
				sources_lines.append(f"S{i}: {title} ‚Äî {url}")
			fs_block = ("FACTS:\n" + "\n".join(facts_lines)) + ("\n\n" + ("SOURCES:\n" + "\n".join(sources_lines)) if sources_lines else "")
			messages.append({"role": "system", "content": fs_block})
		except Exception:
			pass
	if ctx_text:
		# Inject rate lines separately to anchor exact numbers; instruct to cite with [S#]
		rate_block = "\n".join(rag_meta.get("rates", []) or [])
		add = "–°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏; –≤ –æ—Ç–≤–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Å—ã–ª–∫–∏ [S#] –Ω–∞ SOURCES, URL –Ω–µ –≤—Å—Ç–∞–≤–ª—è–π –Ω–∞–ø—Ä—è–º—É—é):\n" + ctx_text
		if rate_block:
			add += "\n\n–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å–æ —Å—Ç–∞–≤–∫–∞–º–∏ (–∏—Å–ø–æ–ª—å–∑—É–π –¥–æ—Å–ª–æ–≤–Ω–æ –∏ –≤—Å–µ–≥–¥–∞ —É–∫–∞–∑—ã–≤–∞–π –≤–∞–ª—é—Ç—É):\n" + rate_block
		messages.append({"role": "system", "content": add})
	# Keep last chat history minimal to avoid polluting topic; include last 10
	history = db.get_assistant_messages(tg_id, limit=10)
	for m in history:
		messages.append({"role": m["role"], "content": m["content_sanitized"]})
	messages.append({"role": "user", "content": user_clean})

	resp = client.chat.completions.create(
		model="gpt-4o-mini",
		messages=messages,
		temperature=0.3,
		max_tokens=350,
	)
	answer = resp.choices[0].message.content or ""
	answer_clean = sanitize_text_assistant_output(answer)
	answer_clean = _normalize_bullets(answer_clean)
	answer_clean = _strip_md_emphasis(answer_clean)

	# Store
	db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
	db.add_assistant_message(tg_id, "assistant", answer_clean, off_topic=False)
	return answer_clean 