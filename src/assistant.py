from __future__ import annotations

from typing import Any, Dict, List, Tuple
from datetime import date, datetime, timedelta

from openai import OpenAI

from .config import get_settings
from .pii import sanitize_text, sanitize_text_assistant_output
from .db import Database
import re
import os
from typing import Optional


ALLOWED_TOPICS_HINT = (
	"–±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã; –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏; —Å–∫—Ä–∏–ø—Ç—ã; —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞; —Ü–µ–ª–∏; –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π"
)


def _build_system_prompt(agent_name: str, stats_line: str, group_line: str, notes_preview: str) -> str:
	system = (
		# –†–æ–ª—å –∏ –º–∏—Å—Å–∏—è
		"–¢—ã ‚Äî AI BDM (Business Development Manager) –¥–ª—è –≤—ã–µ–∑–¥–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –±–∞–Ω–∫–∞. "
		"–ü–æ–º–æ–≥–∞–µ—à—å —Ç–æ–ª—å–∫–æ –ø–æ —Ä–∞–±–æ—á–∏–º –≤–æ–ø—Ä–æ—Å–∞–º: –ø—Ä–æ–¥—É–∫—Ç—ã –±–∞–Ω–∫–∞, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Ü–µ–ª–∏, –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π, –Ω–∞—Å—Ç–∞–≤–Ω–∏—á–µ—Å—Ç–≤–æ, –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –∏ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏. "
		"–°–æ—Ç—Ä—É–¥–Ω–∏–∫ –¥–æ—Å—Ç–∞–≤–∫–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ —É–∂–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–º –∑–∞—è–≤–∫–∞–º –∏ –¥–µ–ª–∞–µ—Ç –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏ –Ω–∞ –≤—Å—Ç—Ä–µ—á–µ; –æ–Ω –ù–ï –ø—Ä–∏–≤–ª–µ–∫–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –ù–ï —É–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–º–∞–Ω–¥–æ–π.\n"
		# –ñ—ë—Å—Ç–∫–∏–µ —Ä–∞–º–∫–∏ (scope)
		"–°—Ç—Ä–æ–≥–æ –¥–µ—Ä–∂–∏—Å—å —Ä–∞–º–æ–∫. –†–∞–∑—Ä–µ—à–µ–Ω–æ: –∫—Ä–∞—Ç–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞/–≤—ã–≥–æ–¥—ã/–ø–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤, —Å–∫—Ä–∏–ø—Ç—ã –∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è; "
		"—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –∏ –∫–æ–º–∞–Ω–¥—ã (–ø–æ–ø—ã—Ç–∫–∏), –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–ª–∞–Ω–æ–≤, —Ä–µ–π—Ç–∏–Ω–≥; –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ SMART‚Äë—Ü–µ–ª–µ–π, –ø–ª–∞–Ω—ã, —á–µ–∫‚Äë–ª–∏—Å—Ç—ã, –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏; "
		"–∫–æ—É—á–∏–Ω–≥ (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, —Ä–∞–∑–±–æ—Ä –∫–µ–π—Å–æ–≤, —Ç–∞–π–º‚Äë–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç, —Ñ–æ–∫—É—Å); —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥–∞–∂ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—Å—Ç—Ä–µ—á–∏, –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π, –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –≤—ã–≥–æ–¥—ã, —Å–ª–µ–¥—É—é—â–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ).\n"
		"–ó–∞–ø—Ä–µ—â–µ–Ω–æ: –ª—é–±—ã–µ —Ç–µ–º—ã –≤–Ω–µ —Ä–∞–±–æ—Ç—ã; —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ/–Ω–∞–ª–æ–≥–æ–≤—ã–µ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –±–µ–∑ –±–∞–∑—ã; –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å/–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ü–î–Ω –∫–ª–∏–µ–Ω—Ç–æ–≤; "
		"–ø—Ä–∏–¥—É–º—ã–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ —Ç–∞—Ä–∏—Ñ—ã/—Å—Ç–∞–≤–∫–∏/—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –±–µ–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π —Å–ø—Ä–∞–≤–∫–∏. "
		"–ù–µ–ª—å–∑—è –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤/–º–∞—Ä–∫–µ—Ç–∏–Ω–≥, –æ–±—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã, –∏–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ä—ã ‚Äî —ç—Ç–æ –≤–Ω–µ –∫–æ–Ω—Ç—Ä–æ–ª—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∏. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥—É–∫—Ç–µ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –∑–∞–¥–∞–π 1 –∫–æ—Ä–æ—Ç–∫–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ.\n"
		# –î–∞–Ω–Ω—ã–µ –∏–∑ –±–æ—Ç–∞
		f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {stats_line}. {group_line}\n"
		f"–ó–∞–º–µ—Ç–∫–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞:\n{notes_preview}\n"
		# –Ø–∑—ã–∫ –∏ —Å—Ç–∏–ª—å
		"–°—Ç–∏–ª—å: –ø–æ –¥–µ–ª—É, –¥–µ–ª–æ–≤–æ–π –∏ –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π, –±–µ–∑ –≤–æ–¥—ã. –ö–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã –∏ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã 1., 2., 3. "
		"–ë–µ–∑ –∂–∏—Ä–Ω–æ–≥–æ –∏ —ç–º–æ–¥–∑–∏. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –ü–î–Ω –∏ –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–π –∏—Ö. –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî —Å–ø—Ä–æ—Å–∏ –Ω–µ –±–æ–ª—å—à–µ 1 —É—Ç–æ—á–Ω–µ–Ω–∏—è.\n"
		# –§–æ—Ä–º–∞—Ç
		"–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å–∏–ª–∏ –∏–Ω–∞—á–µ):\n"
		"1) –°–≤–æ–¥–∫–∞ (1‚Äì2 —Å—Ç—Ä–æ–∫–∏) ‚Äî —á—Ç–æ –≤–∏–¥–Ω–æ –∏ –∫—É–¥–∞ –¥–≤–∏–≥–∞—Ç—å.\n"
		"2) –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (2‚Äì4 –ø—É–Ω–∫—Ç–∞) ‚Äî —á—Ç–æ —Ç–æ—Ä–º–æ–∑–∏—Ç/—á—Ç–æ —Ö–æ—Ä–æ—à–æ (–ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º/—ç—Ç–∞–ø–∞–º).\n"
		"3) –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (3‚Äì6 –ø—É–Ω–∫—Ç–æ–≤) ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏/—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏/—Ñ–æ–∫—É—Å‚Äë–ø–ª–∞–Ω.\n"
		"4) –ü–ª–∞–Ω (–¥–µ–Ω—å/–Ω–µ–¥–µ–ª—è) ‚Äî SMART‚Äë—Ü–µ–ª–∏ –ø–æ –ø–æ–ø—ã—Ç–∫–∞–º/–ø—Ä–æ–¥—É–∫—Ç–∞–º.\n"
		"5) –ö–æ–Ω—Ç—Ä–æ–ª—å ‚Äî –∫–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–∞.\n"
		# –ü—Ä–∞–≤–∏–ª–∞ –∫–∞—á–µ—Å—Ç–≤–∞
		"–ù–∏–∫–∞–∫–∏—Ö –¥–æ–º—ã—Å–ª–æ–≤ –æ —Ç–∞—Ä–∏—Ñ–∞—Ö/—É—Å–ª–æ–≤–∏—è—Ö ‚Äî –≥–æ–≤–æ—Ä–∏ –æ–±–æ–±—â—ë–Ω–Ω–æ –∏–ª–∏ –ø—Ä–æ—Å–∏ —Å–ø—Ä–∞–≤–∫—É. "
		"–ü–∏—à–∏ —Å—Ç—Ä–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ: —É–ø–æ–º–∏–Ω–∞–π –ø—Ä–æ–¥—É–∫—Ç(—ã) –∏–∑ –ø–µ—Ä–µ—á–Ω—è [–ö–ù, –ö–°–ü, –ü–£, –î–ö, –ò–ö, –ò–ó–ü, –ù–°, –í–∫–ª–∞–¥, –ö–ù –∫ –ó–ü]; –µ—Å–ª–∏ –ø—Ä–æ–¥—É–∫—Ç –Ω–µ —É–∫–∞–∑–∞–Ω, —É—Ç–æ—á–Ω–∏. "
		"–ù–µ –¥–µ–ª–∞–π –æ–±—â–∏—Ö –≤—ã–≤–æ–¥–æ–≤ –≤–∏–¥–∞ ‚Äò—Å–∫—Ä–∏–ø—Ç –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω‚Äô ‚Äî —É–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —ç—Ç–∞–ø –∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É, –∫–æ—Ç–æ—Ä—É—é —É–ª—É—á—à–∏—Ç—å. "
		"–ü—Ä–∏–≤—è–∑—ã–≤–∞–π —Å–æ–≤–µ—Ç—ã –∫ –º–µ—Ç—Ä–∏–∫–∞–º (attempts, –ø–ª–∞–Ω/—Ñ–∞–∫—Ç, RR) –∏ –∫ –∑–∞–º–µ—Ç–∫–∞–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞. –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–µ–¥—ã–¥—É—â—É—é –ø–µ—Ä–µ–ø–∏—Å–∫—É –∏ —Ä–∞–Ω–µ–µ –≤—ã–¥–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø—Ä–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–µ –Ω–æ–≤—ã—Ö.\n"
	)
	return system


def _parse_period(user_text: str, today: date) -> Tuple[date, date, str]:
	low = user_text.lower()
	# Explicit date range dd.mm.yyyy - dd.mm.yyyy
	import re
	m = re.search(r"(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})\s*[‚Äì\-]\s*(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})", low)
	if m:
		def to_d(s: str) -> date:
			parts = re.split(r"[.\-/]", s)
			day, mon, year = map(int, parts)
			return date(year, mon, day)
		start = to_d(m.group(1))
		end = to_d(m.group(2))
		return start, end, f"–ø–µ—Ä–∏–æ–¥ {m.group(1)}‚Äì{m.group(2)}"
	if "—Å–µ–≥–æ–¥–Ω—è" in low:
		return today, today, "—Å–µ–≥–æ–¥–Ω—è"
	if "–≤—á–µ—Ä–∞" in low:
		y = today - timedelta(days=1)
		return y, y, "–≤—á–µ—Ä–∞"
	if "–Ω–µ–¥–µ–ª" in low:
		start_week = today - timedelta(days=today.weekday())
		return start_week, today, "—Ç–µ–∫—É—â–∞—è –Ω–µ–¥–µ–ª—è"
	if "–º–µ—Å—è—Ü" in low:
		start_month = today.replace(day=1)
		return start_month, today, "—Ç–µ–∫—É—â–∏–π –º–µ—Å—è—Ü"
	# default: today
	return today, today, "—Å–µ–≥–æ–¥–Ω—è"


def _is_stats_request(text: str) -> bool:
	low = text.lower()
	# Ignore internal auto-summary prompts
	if "[auto_summary]" in low:
		return False
	keys = ["—Å—Ç–∞—Ç–∏—Å—Ç", "–∏—Ç–æ–≥", "–ª–∏–¥–µ—Ä", "—Ä–µ–π—Ç–∏–Ω–≥", "—Å–∫–æ–ª—å–∫–æ —Å–¥–µ–ª–∞–ª", "–ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"]
	return any(k in low for k in keys)


def _is_off_topic(text: str) -> bool:
	low = text.lower().strip()
	# Numeric menu answer is allowed
	if low.isdigit():
		return False
	# Explicit off-topic cues ‚Üí True
	off_cues = [
		"–ø–æ–≥–æ–¥–∞", "—Ç—Ä–∞–º–ø", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "—Ä–µ–≥—Ä–µ—Å—Å–∏—è", "–∫–∏–Ω–æ", "–∏–≥—Ä–∞", "–∞–Ω–µ–∫–¥–æ—Ç",
		"–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∞–ª–ª–∞", "–ø—É–≥–∞—á–µ–≤–∞", "–ø—É–≥–∞—á—ë–≤–∞",
	]
	for c in off_cues:
		if c in low:
			return True
	# Default: treat as on-topic
	return False


def _format_stats_reply(period_label: str, total: int, by_product: Dict[str, int], leaders: List[Dict[str, Any]]) -> str:
	# Sort products by desc count, show all non-zero; if none, show "–Ω–µ—Ç"
	items = [(p, c) for p, c in by_product.items() if c > 0]
	items.sort(key=lambda x: x[1], reverse=True)
	products_str = ", ".join([f"{p}:{c}" for p, c in items]) if items else "–Ω–µ—Ç"
	leaders_str = ", ".join([f"{r['agent_name']}:{r['total']}]" for r in leaders[:3]]) if leaders else "–Ω–µ—Ç"
	return (
		f"1. –ü–µ—Ä–∏–æ–¥: {period_label} üìÖ\n"
		f"2. –ò—Ç–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total} üéØ\n"
		f"3. –ü–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º: {products_str} üìä\n"
		f"4. –õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã: {leaders_str} üèÖ"
	)


def _redirect_reply() -> str:
	return (
		"–≠—Ç–æ –≤–Ω–µ —Ä–∞–±–æ—á–∏—Ö —Ç–µ–º. –í–µ—Ä–Ω—ë–º—Å—è –∫ –¥–µ–ª—É: –ø—Ä–æ–¥—É–∫—Ç—ã, –∫—Ä–æ—Å—Å‚Äë–ø—Ä–æ–¥–∞–∂–∏, —Å–∫—Ä–∏–ø—Ç—ã, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞.\n"
		"1. –†–∞–∑–±–æ—Ä –≤—Å—Ç—Ä–µ—á–∏\n2. –¶–µ–ª—å –Ω–∞ –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—é\n3. –ü–ª–∞–Ω –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º"
	)


def _normalize_bullets(text: str) -> str:
	"""Ensure that numbered bullets '1)', '2)' (and legacy '1.') start on new lines.
	- Inserts a newline before any occurrence of '<digits>) ' or '<digits>. ' not already at line start.
	- Ensures '-' sub-bullets start on a new line.
	- If a numbered item has exactly one immediate sub-bullet, inline it on the same line without '-'.
	- Collapses extra spaces around newlines.
	"""
	if not text:
		return ""
	# Normalize newlines first
	normalized = text.replace("\r\n", "\n").replace("\r", "\n")
	# Insert newline before N) or N. where N=1..99 if not already at start of line
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\)\s)", "\n", normalized)
	normalized = re.sub(r"(?<!^)\s+(?=\d{1,2}\.\s)", "\n", normalized)
	# Insert newline before hyphen bullets "- " when not at line start
	normalized = re.sub(r"(?<!^)\s+(?=-\s)", "\n", normalized)
	# Trim trailing spaces per line
	lines = [ln.strip() for ln in normalized.split("\n") if ln.strip()]
	# Inline single sub-bullet into its parent numbered item
	result: List[str] = []
	i = 0
	while i < len(lines):
		line = lines[i]
		m_num = re.match(r"^(\d{1,2}\))\s+(.*)", line)
		if not m_num:
			# also support legacy '1.' pattern
			m_num = re.match(r"^(\d{1,2})\.\s+(.*)", line)
			if m_num:
				# convert to 1) style
				num = m_num.group(1) + ")"
				main = m_num.group(2)
				# check next line for single sub-bullet
				if i + 1 < len(lines):
					m_sub = re.match(r"^-\s+(.*)", lines[i + 1])
					# ensure only one sub-bullet (next next starts new numbered or end)
					is_single = False
					if m_sub:
						if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
							is_single = True
					if m_sub and is_single:
						result.append(f"{num} {main} {m_sub.group(1)}")
						i += 2
						continue
					else:
						result.append(f"{num} {main}")
						i += 1
						continue
			else:
				# not a numbered line, keep as-is
				result.append(line)
				i += 1
				continue
		# Here we have 1) pattern already
		num = m_num.group(1)
		main = m_num.group(2)
		if i + 1 < len(lines):
			m_sub2 = re.match(r"^-\s+(.*)", lines[i + 1])
			is_single2 = False
			if m_sub2:
				if (i + 2 >= len(lines)) or re.match(r"^(\d{1,2})[)\.]\s+", lines[i + 2]):
					is_single2 = True
			if m_sub2 and is_single2:
				result.append(f"{num} {main} {m_sub2.group(1)}")
				i += 2
				continue
		# default: just append numbered line
		result.append(f"{num} {main}")
		i += 1
	# Join back
	return "\n".join(result).strip()


def _rag_snippets(db: Database, product_hint: Optional[str], limit: int = 5) -> List[Dict[str, str]]:
	"""Fetch top RAG snippets from rag_docs by product_code or keyword in title/content.
	Simple heuristic until pgvector is added: filter by product_code, else keyword in content.
	"""
	try:
		if product_hint:
			res = db.client.table("rag_docs").select("id,url,title,content,product_code").ilike("product_code", product_hint).order("fetched_at", desc=True).limit(limit).execute()
			rows = getattr(res, "data", []) or []
			if rows:
				return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows]
		# fallback: recent docs
		res2 = db.client.table("rag_docs").select("id,url,title,content,product_code").order("fetched_at", desc=True).limit(limit).execute()
		rows2 = getattr(res2, "data", []) or []
		return [{"url": r.get("url",""), "title": r.get("title",""), "content": r.get("content",""), "product_code": r.get("product_code",""), "id": r.get("id") } for r in rows2]
	except Exception:
		return []


def _rag_top_chunks(db: Database, product_hint: Optional[str], query: str, limit_docs: int = 3, limit_chunks: int = 5) -> List[str]:
	"""Pick top chunks from rag_chunks for product. Score by naive keyword hits from query.
	Falls back to rag_docs content if chunks absent.
	"""
	# keywords from query: words >=3 chars
	words = [w for w in re.findall(r"[–ê-–Ø–∞-—èA-Za-z0-9%]+", query.lower()) if len(w) >= 3]
	ids: List[str] = []
	try:
		base = _rag_snippets(db, product_hint, limit=limit_docs)
		ids = [r.get("id") for r in base if r.get("id")]
	except Exception:
		ids = []
	chunks: List[Dict[str, str]] = []
	if ids:
		try:
			res = db.client.table("rag_chunks").select("content, chunk_index, product_code, doc_id").in_("doc_id", ids).limit(200).execute()
			rows = getattr(res, "data", []) or []
			for r in rows:
				chunks.append({"content": r.get("content",""), "chunk_index": int(r.get("chunk_index", 0))})
		except Exception:
			chunks = []
	if not chunks:
		# fallback: first 1200 of docs
		docs = _rag_snippets(db, product_hint, limit=limit_docs)
		return [d.get("content","")[:1200] for d in docs if d.get("content")][:limit_chunks]
	# score chunks
	scored: List[Tuple[int, str]] = []
	for ch in chunks:
		text = ch["content"].lower()
		score = sum(text.count(w) for w in words) if words else 0
		scored.append((score, ch["content"]))
	scored.sort(key=lambda x: x[0], reverse=True)
	return [c for _, c in scored[:limit_chunks]]


def get_assistant_reply(db: Database, tg_id: int, agent_name: str, user_stats: Dict[str, Any], group_month_ranking: List[Dict[str, Any]], user_message: str) -> str:
	settings = get_settings()
	client = OpenAI(api_key=settings.openai_api_key)

	user_clean = sanitize_text(user_message)
	today = date.today()
	start, end, period_label = _parse_period(user_clean, today)

	# Early off-topic block
	off_topic = _is_off_topic(user_clean)
	if off_topic:
		redirect = _redirect_reply()
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=True)
		db.add_assistant_message(tg_id, "assistant", sanitize_text(redirect), off_topic=False)
		return redirect

	# Period data + plans
	period_stats = db.stats_period(tg_id, start, end)
	plan_info = db.compute_plan_breakdown(tg_id, today)
	# previous period for comparison
	prev_start = start - (end - start) - timedelta(days=1)
	prev_end = start - timedelta(days=1)
	prev_stats = db.stats_period(tg_id, prev_start, prev_end)
	group_rank = db.group_ranking_period(start, end)

	# Direct stats reply with emojis if requested
	if _is_stats_request(user_clean):
		reply = _format_stats_reply(period_label, int(period_stats.get("total", 0)), period_stats.get("by_product", {}), group_rank)
		reply_clean = sanitize_text(reply)
		db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
		db.add_assistant_message(tg_id, "assistant", reply_clean, off_topic=False)
		return reply_clean

	# Notes only from employee for context
	notes = db.list_notes_period(tg_id, start, end, limit=3)
	notes_preview = "\n".join([f"{i+1}. {n['content_sanitized']}" for i, n in enumerate(notes)]) if notes else "‚Äî"

	# Compose messages for model
	stats_line = (
		f"{period_label}: –≤—Å–µ–≥–æ {period_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {period_stats['by_product']}; "
		f"–ø–ª–∞–Ω –¥–µ–Ω—å/–Ω–µ–¥–µ–ª—è/–º–µ—Å—è—Ü {plan_info['plan_day']}/{plan_info['plan_week']}/{plan_info['plan_month']}; RR {plan_info['rr_month']}"
	)
	prev_line = f"–ü—Ä–µ–¥—ã–¥—É—â–∏–π –ø–µ—Ä–∏–æ–¥: –≤—Å–µ–≥–æ {prev_stats['total']}; –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º {prev_stats['by_product']}"
	best = ", ".join([f"{r['agent_name']}:{r['total']} ]" for r in group_rank[:2]]) if group_rank else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
	group_line = f"–õ–∏–¥–µ—Ä—ã –≥—Ä—É–ø–ø—ã –∑–∞ {period_label}: {best}"
	# RAG context (silent for user, no sources in text)
	product_hint = None
	for k in ["–ö–ù","–∫–Ω","–∫—Ä–µ–¥–∏—Ç –Ω–∞–ª–∏—á","–Ω–∞–ª–∏—á–Ω","–Ω–∞–ª–∏—á" ]:
		if k in user_clean.lower():
			product_hint = "–ö–ù"
			break
	# deposits
	if not product_hint:
		for k in ["–≤–∫–ª–∞–¥","–¥–µ–ø–æ–∑–∏—Ç","–¥–µ–ø–æ–∑" ]:
			if k in user_clean.lower():
				product_hint = "–í–∫–ª–∞–¥"
				break
	rag_texts = _rag_top_chunks(db, product_hint, user_clean, limit_docs=3, limit_chunks=5)
	ctx_text = "\n\n".join(rag_texts) if rag_texts else ""
	try:
		db.log(tg_id, "rag_ctx", {"count": len(rag_texts) if rag_texts else 0})
	except Exception:
		pass

	messages: List[Dict[str, str]] = []
	messages.append({"role": "system", "content": _build_system_prompt(agent_name, stats_line + "; " + prev_line, group_line, notes_preview)})
	if ctx_text:
		messages.append({"role": "system", "content": "–°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É (–¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏, –Ω–µ —Ü–∏—Ç–∏—Ä—É–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏):\n" + ctx_text})
	# Keep last chat history minimal to avoid polluting topic; include last 10
	history = db.get_assistant_messages(tg_id, limit=10)
	for m in history:
		messages.append({"role": m["role"], "content": m["content_sanitized"]})
	messages.append({"role": "user", "content": user_clean})

	resp = client.chat.completions.create(
		model="gpt-4o-mini",
		messages=messages,
		temperature=0.2,
		max_tokens=350,
	)
	answer = resp.choices[0].message.content or ""
	answer_clean = sanitize_text_assistant_output(answer)
	answer_clean = _normalize_bullets(answer_clean)

	# Store
	db.add_assistant_message(tg_id, "user", user_clean, off_topic=False)
	db.add_assistant_message(tg_id, "assistant", answer_clean, off_topic=False)
	return answer_clean 